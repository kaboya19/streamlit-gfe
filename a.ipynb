{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping URL: https://www.migros.com.tr/arama?q=pirin%C3%A7&kategori=5&sirala=indirim-yuzdesine-gore&sayfa=\n",
      "Product: Tadıbol Baldo Pirinç 2,5 Kg, Price: 199.95 TRY\n",
      "Product: M Life Bio Organik Pirinç 1000 G, Price: 85.9 TRY\n",
      "Product: Migros Baldo Pirinç 1000 G, Price: 67.5 TRY\n",
      "Product: Migros Pilavlık İthal Pirinç 1000 G, Price: 31.5 TRY\n",
      "Product: Migros Osmancık Pirinç 1000 G, Price: 51.95 TRY\n",
      "Product: Migros Kırık Pirinç 1000 G, Price: 21.9 TRY\n",
      "Product: Migros Jasmine Pirinç 1000 G, Price: 44.5 TRY\n",
      "Product: Migros Osmancık Pirinç 2500 G, Price: 117.5 TRY\n",
      "Product: Migros Baldo Pirinç 2500 G, Price: 162.5 TRY\n",
      "Product: M Life Basmati Pirinç 500 G, Price: 39.9 TRY\n",
      "Product: Migros Yerli Pilavlık Pirinç 2.5 Kg, Price: 104.5 TRY\n",
      "Product: Migros İthal Pilavlık Pirinç 2.5 Kg, Price: 71.5 TRY\n",
      "Product: Hasata Gönen Baldo Pirinç 2 Kg, Price: 215.95 TRY\n",
      "Product: Tat Baldo Pirinç 1 Kg, Price: 101.95 TRY\n",
      "Product: Yayla Gurme Siyah Pirinç 500 G, Price: 70.95 TRY\n",
      "Product: Reis Gönen Baldo 2,5 Kg, Price: 334.95 TRY\n",
      "Product: Ekin Osmancık Pirinç 1 Kg, Price: 74.95 TRY\n",
      "Product: Reis Royal Siyah Pirinç 500 G, Price: 121.95 TRY\n",
      "Product: Yayla Pilavlık Pirinç 1 Kg, Price: 33.95 TRY\n",
      "Product: Yayla Baldo Pirinç 2 Kg Gönen Bölgesi Mahsulü, Price: 198.95 TRY\n",
      "Product: Tat Pilavlık İthal Pirinç 1 Kg, Price: 52.95 TRY\n",
      "Product: Tat Yasemin Pirinç 1 Kg, Price: 72.95 TRY\n",
      "Product: Yayla Gurme Jasmine Pirinç 500 G, Price: 46.95 TRY\n",
      "Product: Yayla Gurme Basmati Pirinç 500 G, Price: 58.95 TRY\n",
      "Product: Yayla Trakya Baldo Pirinç 2 Kg, Price: 179.95 TRY\n",
      "Product: Yayla Yerli Pirinç Osmancık Bölgesi Mahsulü 2 Kg, Price: 149.95 TRY\n",
      "Product: Reis Basmati Pirinç 1 Kg, Price: 172.95 TRY\n",
      "Product: Tat Osmancık Pirinç 2 Kg, Price: 146.95 TRY\n",
      "Product: Reis Royal Basmati Pirinç (Superıor) 500 G, Price: 121.95 TRY\n",
      "Product: Reis Royal Kepekli Pirinç 500 G, Price: 106.95 TRY\n",
      "Scraping URL: https://www.migros.com.tr/arama?q=pirin%C3%A7&kategori=5&sirala=indirim-yuzdesine-gore&sayfa=2\n",
      "Product: Tat Basmati Pirinç 1 Kg, Price: 97.95 TRY\n",
      "Product: Tat Osmancık Pirinç 1 Kg, Price: 75.95 TRY\n",
      "Product: Reis Osmancık Pirinç 1 Kg, Price: 124.95 TRY\n",
      "Product: Tat Baldo Pirinç 2 Kg, Price: 189.95 TRY\n",
      "Product: Yayla Osmancık Bölgesi Mahsulü Yerli Pirinç 1 Kg, Price: 77.95 TRY\n",
      "Product: Anadolu Lezzetleri Sarıköy Gönen Baldo Pirinç 1 Kg, Price: 87.9 TRY\n",
      "Product: Hasata Gönen Baldo Pirinç 1 Kg, Price: 109.95 TRY\n",
      "Product: Yayla Gurme Risotto Pirinç 500 G, Price: 72.95 TRY\n",
      "Product: Anadolu Lezzetleri Tosya Sarıkılçık Pirinç 500 G, Price: 85.95 TRY\n",
      "Product: Yayla Gurme Sushi Pirinci 500 G, Price: 49.95 TRY\n",
      "Product: Reis Jasmin Pirinç 1 Kg, Price: 165.95 TRY\n",
      "Product: Yayla Baldo Pirinç 1 Kg Gönen Bölgesi Mahsulü, Price: 99.95 TRY\n",
      "Product: Yayla Gurme Kepekli Pirinç 500 G, Price: 44.95 TRY\n",
      "Product: Ekin Baldo Pirinç 1 Kg, Price: 96.95 TRY\n",
      "Product: Hasata Osmancık Pirinç 1 Kg, Price: 86.95 TRY\n",
      "Product: Yayla Tane Tane Pirinç 2 Kg, Price: 136.95 TRY\n",
      "Product: Hasata Osmancık Pirinç 2 Kg, Price: 171.95 TRY\n",
      "Product: Reis Arborio Risotto Vakumlu Pirinç 500 G, Price: 123.95 TRY\n",
      "Product: Reis Gönen Baldo Pirinc 1 Kg, Price: 149.95 TRY\n",
      "Product: Yayla Kırık Pirinç 1 Kg, Price: 31.95 TRY\n",
      "Product: S:S Tire Organik Pirinç 1 Kg, Price: 99.95 TRY\n",
      "Product: Yayla Uzun Tane Pirinç 2 Kg, Price: 99.95 TRY\n",
      "Product: Yayla Tane Tane Pirinç 1 Kg, Price: 71.95 TRY\n",
      "Product: Reis Osmancık Pirinç 2,5 Kg, Price: 288.95 TRY\n",
      "Product: Hasata Gönen Baldo Pirinç 2 Kg, Price: 215.95 TRY\n",
      "Product: Tat Baldo Pirinç 1 Kg, Price: 101.95 TRY\n",
      "HTTPSConnectionPool(host='www.carrefoursa.com', port=443): Max retries exceeded with url: /pirinc/c/1134?q=%3AbestSeller%3AinStockFlag%3Atrue&text= (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000229C7A785E0>, 'Connection to www.carrefoursa.com timed out. (connect timeout=None)'))\n",
      "Scraping URL: https://www.migros.com.tr/arama?q=pirin%C3%A7&kategori=5&sirala=indirim-yuzdesine-gore&sayfa=\n",
      "Product: Tadıbol Baldo Pirinç 2,5 Kg, Price: 199.95 TRY\n",
      "Product: M Life Bio Organik Pirinç 1000 G, Price: 85.9 TRY\n",
      "Product: Migros Baldo Pirinç 1000 G, Price: 67.5 TRY\n",
      "Product: Migros Pilavlık İthal Pirinç 1000 G, Price: 31.5 TRY\n",
      "Product: Migros Osmancık Pirinç 1000 G, Price: 51.95 TRY\n",
      "Product: Migros Kırık Pirinç 1000 G, Price: 21.9 TRY\n",
      "Product: Migros Jasmine Pirinç 1000 G, Price: 44.5 TRY\n",
      "Product: Migros Osmancık Pirinç 2500 G, Price: 117.5 TRY\n",
      "Product: Migros Baldo Pirinç 2500 G, Price: 162.5 TRY\n",
      "Product: M Life Basmati Pirinç 500 G, Price: 39.9 TRY\n",
      "Product: Migros Yerli Pilavlık Pirinç 2.5 Kg, Price: 104.5 TRY\n",
      "Product: Migros İthal Pilavlık Pirinç 2.5 Kg, Price: 71.5 TRY\n",
      "Product: Hasata Gönen Baldo Pirinç 2 Kg, Price: 215.95 TRY\n",
      "Product: Tat Baldo Pirinç 1 Kg, Price: 101.95 TRY\n",
      "Product: Yayla Gurme Siyah Pirinç 500 G, Price: 70.95 TRY\n",
      "Product: Reis Gönen Baldo 2,5 Kg, Price: 334.95 TRY\n",
      "Product: Ekin Osmancık Pirinç 1 Kg, Price: 74.95 TRY\n",
      "Product: Reis Royal Siyah Pirinç 500 G, Price: 121.95 TRY\n",
      "Product: Yayla Pilavlık Pirinç 1 Kg, Price: 33.95 TRY\n",
      "Product: Yayla Baldo Pirinç 2 Kg Gönen Bölgesi Mahsulü, Price: 198.95 TRY\n",
      "Product: Tat Pilavlık İthal Pirinç 1 Kg, Price: 52.95 TRY\n",
      "Product: Tat Yasemin Pirinç 1 Kg, Price: 72.95 TRY\n",
      "Product: Yayla Gurme Jasmine Pirinç 500 G, Price: 46.95 TRY\n",
      "Product: Yayla Gurme Basmati Pirinç 500 G, Price: 58.95 TRY\n",
      "Product: Yayla Trakya Baldo Pirinç 2 Kg, Price: 179.95 TRY\n",
      "Product: Yayla Yerli Pirinç Osmancık Bölgesi Mahsulü 2 Kg, Price: 149.95 TRY\n",
      "Product: Reis Basmati Pirinç 1 Kg, Price: 172.95 TRY\n",
      "Product: Tat Osmancık Pirinç 2 Kg, Price: 146.95 TRY\n",
      "Product: Reis Royal Basmati Pirinç (Superıor) 500 G, Price: 121.95 TRY\n",
      "Product: Reis Royal Kepekli Pirinç 500 G, Price: 106.95 TRY\n",
      "Scraping URL: https://www.migros.com.tr/arama?q=pirin%C3%A7&kategori=5&sirala=indirim-yuzdesine-gore&sayfa=2\n",
      "Product: Tat Basmati Pirinç 1 Kg, Price: 97.95 TRY\n",
      "Product: Tat Osmancık Pirinç 1 Kg, Price: 75.95 TRY\n",
      "Product: Reis Osmancık Pirinç 1 Kg, Price: 124.95 TRY\n",
      "Product: Tat Baldo Pirinç 2 Kg, Price: 189.95 TRY\n",
      "Product: Yayla Osmancık Bölgesi Mahsulü Yerli Pirinç 1 Kg, Price: 77.95 TRY\n",
      "Product: Anadolu Lezzetleri Sarıköy Gönen Baldo Pirinç 1 Kg, Price: 87.9 TRY\n",
      "Product: Hasata Gönen Baldo Pirinç 1 Kg, Price: 109.95 TRY\n",
      "Product: Yayla Gurme Risotto Pirinç 500 G, Price: 72.95 TRY\n",
      "Product: Anadolu Lezzetleri Tosya Sarıkılçık Pirinç 500 G, Price: 85.95 TRY\n",
      "Product: Yayla Gurme Sushi Pirinci 500 G, Price: 49.95 TRY\n",
      "Product: Reis Jasmin Pirinç 1 Kg, Price: 165.95 TRY\n",
      "Product: Yayla Baldo Pirinç 1 Kg Gönen Bölgesi Mahsulü, Price: 99.95 TRY\n",
      "Product: Yayla Gurme Kepekli Pirinç 500 G, Price: 44.95 TRY\n",
      "Product: Ekin Baldo Pirinç 1 Kg, Price: 96.95 TRY\n",
      "Product: Hasata Osmancık Pirinç 1 Kg, Price: 86.95 TRY\n",
      "Product: Yayla Tane Tane Pirinç 2 Kg, Price: 136.95 TRY\n",
      "Product: Hasata Osmancık Pirinç 2 Kg, Price: 171.95 TRY\n",
      "Product: Reis Arborio Risotto Vakumlu Pirinç 500 G, Price: 123.95 TRY\n",
      "Product: Reis Gönen Baldo Pirinc 1 Kg, Price: 149.95 TRY\n",
      "Product: Yayla Kırık Pirinç 1 Kg, Price: 31.95 TRY\n",
      "Product: S:S Tire Organik Pirinç 1 Kg, Price: 99.95 TRY\n",
      "Product: Yayla Uzun Tane Pirinç 2 Kg, Price: 99.95 TRY\n",
      "Product: Yayla Tane Tane Pirinç 1 Kg, Price: 71.95 TRY\n",
      "Product: Reis Osmancık Pirinç 2,5 Kg, Price: 288.95 TRY\n",
      "Product: Hasata Gönen Baldo Pirinç 2 Kg, Price: 215.95 TRY\n",
      "Product: Tat Baldo Pirinç 1 Kg, Price: 101.95 TRY\n",
      "Scraping URL: https://www.migros.com.tr/arama?q=bu%C4%9Fday%20unu\n",
      "Product: Söke Tam Buğday Unu 1 Kg, Price: 54.95 TRY\n",
      "Product: Sinangil Tam Buğday Unu 1 Kg, Price: 43.95 TRY\n",
      "Product: Söke Tam Buğday Unu 2 Kg, Price: 99.95 TRY\n",
      "Product: Migros Tam Buğday Unu 2 Kg, Price: 61.0 TRY\n",
      "Scraping page 1: https://www.migros.com.tr/arama?q=bebek%20s%C3%BCt%C3%BC&kategori=70507&sirala=akilli-siralama&sayfa=1\n",
      "Product: Bebelac Gold Devam Sütü 3 350 G, Price: 299.95 TRY\n",
      "Product: Aptamil Prosyneo Bebek Sütü 1 400 G, Price: 459.95 TRY\n",
      "Product: Hipp 1 Organik Combiotic Bebek Sütü 350 G, Price: 439.95 TRY\n",
      "Product: Bebelac Gold Devam Sütü 3 800 G, Price: 599.95 TRY\n",
      "Product: Bebelac Gold Devam Sütü 3 800 G + Bebelac Gold 350G, Price: 779.95 TRY\n",
      "Product: Hipp 1 Organik Combiotic Bebek Sütü 600 G, Price: 669.95 TRY\n",
      "Product: Aptamil 1 Bebek Sütü 1200 G 0-6 Ay, Price: 999.95 TRY\n",
      "Product: SMA İyi Geceler 1 0-6 ay Bebek Sütü 800g, Price: 684.95 TRY\n",
      "Product: Aptamil 1 Bebek Sütü İçime Hazır 200 Ml 0-6 Ay, Price: 49.95 TRY\n",
      "Product: Aptamil 1 Bebek Sütü 800 G 0-6 Ay Akıllı Kutu, Price: 729.95 TRY\n",
      "Product: Aptamil 1 Bebek Sütü 350 G 0-6 Ay Akıllı Kutu, Price: 379.95 TRY\n",
      "Product: Bebelac Bebek Devam Sütü 1 400 G, Price: 279.95 TRY\n",
      "Product: Bebelac Gold Bebek Devam Sütü 1 - 800 G, Price: 599.95 TRY\n",
      "Product: Bebelac Gold Bebek Devam Sütü 1 350 G, Price: 299.95 TRY\n",
      "Product: Hipp 1 Organik Keçi Sütü Bazlı Bebek Sütü 400 G, Price: 519.95 TRY\n",
      "Product: Hipp 3 Organik Combiotic Devam Sütü 350 G, Price: 439.95 TRY\n",
      "Product: Bebelac Gold Devam Sütü 2 800G, Price: 599.95 TRY\n",
      "Product: Bebelac Devam Sütü 3 400 G, Price: 279.95 TRY\n",
      "Product: Bebelac Devam Sütü 4 400 G, Price: 279.95 TRY\n",
      "Product: Bebelac Devam Sütü 2 400 G, Price: 279.95 TRY\n",
      "Product: Bebelac Gold Devam Sütü 2 350 G, Price: 299.95 TRY\n",
      "Product: Aptamil Devam Sütü 1 1200 G + 800 G, Price: None TRY\n",
      "Product: Aptamil Çocuk Devam Sütü 4 800 G + Aptamil Çocuk Devam Sütü 4 350 G, Price: 939.95 TRY\n",
      "Product: Aptamil 4 Çocuk Devam Sütü 1200 G 1 Yaş+, Price: 999.95 TRY\n",
      "Product: Sma Optipro Probiyotik 1 Devam Sütü 400 G, Price: 369.95 TRY\n",
      "Product: Bebelac 4 Çocuk Devam Sütü 800+400G 1+ Yaş, Price: 609.95 TRY\n",
      "Product: Aptamil Çocuk Devam Sütü 2 800 G + Aptamil Çocuk Devam Sütü 2 350 G, Price: 939.95 TRY\n",
      "Product: Sma Optipro Probiyotik 1 Devam Sütü 800 G, Price: 663.95 TRY\n",
      "Product: Sma İyi Geceler 2 Devam Sütü 800 G, Price: 684.95 TRY\n",
      "Product: Hipp 2 Organik Combiotic Devam Sütü 350 G, Price: 439.95 TRY\n",
      "Scraping page 2: https://www.migros.com.tr/arama?q=bebek%20s%C3%BCt%C3%BC&kategori=70507&sirala=akilli-siralama&sayfa=2\n",
      "Product: Sma Optipro Probiyotik 2 Devam Sütü 400 G, Price: 369.95 TRY\n",
      "Product: Sma Optipro Probiyotik 3 Devam Sütü 400 G, Price: 369.95 TRY\n",
      "Product: Aptamil Prosyneo Çocuk Devam Sütü 3 400 G, Price: 459.95 TRY\n",
      "Product: Aptamil 5 Çocuk Devam Sütü 800 G 2+ Yaş Akıllı Kutu, Price: 729.95 TRY\n",
      "Product: SMA® Comfort 2 6-12 Ay Devam Sütü 400 G, Price: 472.95 TRY\n",
      "Product: Aptamil 2 Devam Sütü 1200 G 6-9 Ay, Price: 999.95 TRY\n",
      "Product: Aptamil 3 Devam Sütü 1200 G 9-12 Ay, Price: 999.95 TRY\n",
      "Product: SMA® Comfort 3 1-3 Yaş Devam Sütü 400 G, Price: 472.95 TRY\n",
      "Product: Hipp 3 Organik Keçi Sütü Bazlı Devam Sütü 400 G, Price: 519.95 TRY\n",
      "Product: Hipp 2 Organik Keçi Sütü Bazlı Devam Sütü 400 G, Price: 519.95 TRY\n",
      "Product: Aptamil 4 Çocuk Devam Sütü 800 G 1+ Yaş Akıllı Kutu, Price: 729.95 TRY\n",
      "Product: Aptamil 3 Devam Sütü 800 G 9-12 Ay Akıllı Kutu, Price: 729.95 TRY\n",
      "Product: Aptamil 3 Devam Sütü 350 G 9-12 Ay Akıllı Kutu, Price: 379.95 TRY\n",
      "Product: Aptamil Çocuk Devam Sütü İçime Hazır 200 Ml 1 Yaş+, Price: 49.95 TRY\n",
      "Product: Aptamil 2 Devam Sütü İçime Hazır 200 Ml 6-12 Ay, Price: 49.95 TRY\n",
      "Product: Aptamil 2 Devam Sütü 350 G 6-9 Ay Akıllı Kutu, Price: 379.95 TRY\n",
      "name 'carrefour' is not defined\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 125\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# WebDriver'ı başlat\u001b[39;00m\n\u001b[0;32m    124\u001b[0m service \u001b[38;5;241m=\u001b[39m Service(ChromeDriverManager()\u001b[38;5;241m.\u001b[39minstall())\n\u001b[1;32m--> 125\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# Function to clean and convert price text to float\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_price\u001b[39m(price_text):\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;66;03m# Remove any non-numeric characters except for commas and dots\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bora\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[0;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bora\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:55\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     52\u001b[0m     options\u001b[38;5;241m.\u001b[39mbrowser_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m finder\u001b[38;5;241m.\u001b[39mget_driver_path()\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m executor \u001b[38;5;241m=\u001b[39m ChromiumRemoteConnection(\n\u001b[0;32m     58\u001b[0m     remote_server_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mservice_url,\n\u001b[0;32m     59\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mbrowser_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m     ignore_proxy\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m_ignore_local_proxy,\n\u001b[0;32m     63\u001b[0m )\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Bora\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\common\\service.py:106\u001b[0m, in \u001b[0;36mService.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_process_still_running()\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_connectable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# sleep increasing: 0.01, 0.06, 0.11, 0.16, 0.21, 0.26, 0.31, 0.36, 0.41, 0.46, 0.5\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bora\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\common\\service.py:123\u001b[0m, in \u001b[0;36mService.is_connectable\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_connectable\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Establishes a socket connection to determine if the service running\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m    on the port is accessible.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_connectable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bora\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\common\\utils.py:101\u001b[0m, in \u001b[0;36mis_connectable\u001b[1;34m(port, host)\u001b[0m\n\u001b[0;32m     99\u001b[0m socket_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 101\u001b[0m     socket_ \u001b[38;5;241m=\u001b[39m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _is_connectable_exceptions:\n",
      "File \u001b[1;32mc:\\Users\\Bora\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:833\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[0;32m    832\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m--> 833\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[0;32m    835\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime,timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "while True:\n",
    "\n",
    "    try:\n",
    "\n",
    "        bugün=datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "\n",
    "        dün=(datetime.now()-timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "        data=pd.read_csv(\"sepet.csv\")\n",
    "        data=data.set_index(data[\"Unnamed: 0\"]).drop(\"Unnamed: 0\",axis=1)\n",
    "        data.index.name=\"\"\n",
    "        try:\n",
    "            data=data.drop(f\"{bugün}\",axis=1)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "        def veriekle(urun, data, urunler_df):\n",
    "\n",
    "            try:\n",
    "\n",
    "                if urunler_df is None or urunler_df.empty:\n",
    "                    return data\n",
    "\n",
    "                elif isinstance(data.loc[urun], pd.Series):\n",
    "                    data_for_urun = data.loc[urun].to_frame().T  # Convert Series to DataFrame\n",
    "                else:\n",
    "                    data_for_urun = data.loc[urun]\n",
    "\n",
    "                # Merge the data with urunler_df\n",
    "                merged_df = pd.merge(data_for_urun, urunler_df, on='Ürün', how='outer')\n",
    "\n",
    "                # Index'i doğru ürün ismiyle dolduruyoruz\n",
    "                merged_df.index = len(merged_df) * [urun]\n",
    "\n",
    "                # Eğer _x ve _y ile aynı tarihli sütunlar varsa birleştiriyoruz\n",
    "                tarih_sutunlari = [col for col in merged_df.columns if col.endswith(\"_x\") or col.endswith(\"_y\")]\n",
    "                \n",
    "                for col in set([col.split(\"_\")[0] for col in tarih_sutunlari]):\n",
    "                    if col + \"_x\" in merged_df.columns and col + \"_y\" in merged_df.columns:\n",
    "                        # Sütunları birleştiriyoruz\n",
    "                        merged_df[col] = merged_df[col + \"_x\"].combine_first(merged_df[col + \"_y\"])\n",
    "                        # _x ve _y sütunlarını kaldırıyoruz\n",
    "                        merged_df = merged_df.drop(columns=[col + \"_x\", col + \"_y\"])\n",
    "\n",
    "                # Eski verileri (urun'e ait olan satırları) çıkarıyoruz\n",
    "                data_without_urun = data.drop(index=urun)\n",
    "\n",
    "                # Yeni verileri ekliyoruz\n",
    "                data = pd.concat([data_without_urun, merged_df])\n",
    "\n",
    "                # Data'yı index'e göre sıralıyoruz\n",
    "                data = data.sort_index()\n",
    "\n",
    "\n",
    "                return data\n",
    "            except:\n",
    "                return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # Function to scrape product names and prices from Migros for multiple URLs\n",
    "        def scrape_migros_products(pages):\n",
    "            product_data = []\n",
    "            \n",
    "            for page in pages:\n",
    "                print(f\"Scraping URL: {page}\")\n",
    "                driver.get(page)\n",
    "\n",
    "                try:\n",
    "                    # Wait for the product names to load\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '.product-name'))\n",
    "                    )\n",
    "\n",
    "                    # Get product names and prices\n",
    "                    products = driver.find_elements(By.CSS_SELECTOR, '.product-name')\n",
    "                    prices = driver.find_elements(By.CSS_SELECTOR, '.price-new')\n",
    "\n",
    "                    # Ensure we have matching names and prices\n",
    "                    if len(products) != len(prices):\n",
    "                        print(f\"Warning: Number of products and prices do not match on page {page}\")\n",
    "\n",
    "                    for i in range(len(products)):\n",
    "                        try:\n",
    "                            product_name = products[i].text.strip()\n",
    "                            product_price_text = prices[i].text.strip()\n",
    "                            product_price = clean_price(product_price_text)  # Convert to float\n",
    "                            product_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "                            print(f\"Product: {product_name}, Price: {product_price} TRY\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error scraping product {i} on page: {e}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading products on page: {e}\")\n",
    "\n",
    "                # Sleep to avoid being detected as a bot\n",
    "                sleep(2)\n",
    "            \n",
    "            return product_data\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/pirinc/c/1134?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\n",
    "            \"https://www.migros.com.tr/arama?q=pirin%C3%A7&kategori=5&sirala=indirim-yuzdesine-gore&sayfa=\",\"https://www.migros.com.tr/arama?q=pirin%C3%A7&kategori=5&sirala=indirim-yuzdesine-gore&sayfa=2\"\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Pirinç\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            data=veriekle(\"Pirinç\",data,urunler_df)\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # Function to scrape product names and prices from Migros for multiple URLs\n",
    "        def scrape_migros_products(pages):\n",
    "            product_data = []\n",
    "            \n",
    "            for page in pages:\n",
    "                print(f\"Scraping URL: {page}\")\n",
    "                driver.get(page)\n",
    "\n",
    "                try:\n",
    "                    # Wait for the product names to load\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '.product-name'))\n",
    "                    )\n",
    "\n",
    "                    # Get product names and prices\n",
    "                    products = driver.find_elements(By.CSS_SELECTOR, '.product-name')\n",
    "                    prices = driver.find_elements(By.CSS_SELECTOR, '.price-new')\n",
    "\n",
    "                    # Ensure we have matching names and prices\n",
    "                    if len(products) != len(prices):\n",
    "                        print(f\"Warning: Number of products and prices do not match on page {page}\")\n",
    "\n",
    "                    for i in range(len(products)):\n",
    "                        try:\n",
    "                            product_name = products[i].text.strip()\n",
    "                            product_price_text = prices[i].text.strip()\n",
    "                            product_price = clean_price(product_price_text)  # Convert to float\n",
    "                            product_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "                            print(f\"Product: {product_name}, Price: {product_price} TRY\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error scraping product {i} on page : {e}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading products on page : {e}\")\n",
    "\n",
    "                # Sleep to avoid being detected as a bot\n",
    "                sleep(2)\n",
    "            \n",
    "            return product_data\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/search?q=bu%C4%9Fday+unu%3AbestSeller%3AinStockFlag%3Atrue%3AproductPrimaryCategoryCode%3A1276\"]\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/arama?q=bu%C4%9Fday%20unu\"]\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        all_data = migros_data + carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if urunler_df is not None:\n",
    "            urunler_df = product_df.copy()\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Buğday Unu\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            data=veriekle(\"Buğday Unu\",data,urunler_df)\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # Function to scrape product names and prices from Migros\n",
    "        def scrape_migros_products(base_url, total_pages):\n",
    "            product_data = []\n",
    "            \n",
    "            for page in range(1, total_pages + 1):\n",
    "                url = f\"{base_url}{page}\"\n",
    "                print(f\"Scraping page {page}: {url}\")\n",
    "                driver.get(url)\n",
    "\n",
    "                try:\n",
    "                    # Wait for the product names to load\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '.product-name'))\n",
    "                    )\n",
    "\n",
    "                    # Get product names and prices\n",
    "                    products = driver.find_elements(By.CSS_SELECTOR, '.product-name')\n",
    "                    prices = driver.find_elements(By.CSS_SELECTOR, '.price-new')\n",
    "\n",
    "                    # Ensure we have matching names and prices\n",
    "                    if len(products) != len(prices):\n",
    "                        print(f\"Warning: Number of products and prices do not match on page {page}\")\n",
    "\n",
    "                    for i in range(len(products)):\n",
    "                        try:\n",
    "                            product_name = products[i].text.strip()\n",
    "                            product_price_text = prices[i].text.strip()\n",
    "                            product_price = clean_price(product_price_text)  # Convert to float\n",
    "                            product_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "                            print(f\"Product: {product_name}, Price: {product_price} TRY\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error scraping product {i} on page : {e}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading products on page : {e}\")\n",
    "\n",
    "                # Sleep to avoid being detected as a bot\n",
    "                sleep(2)\n",
    "            \n",
    "            return product_data\n",
    "\n",
    "\n",
    "        def scrape_carrefour_products():\n",
    "            carrefour_data=[]\n",
    "            pages=[\"https://www.carrefoursa.com/search?q=devam+s%C3%BCt%C3%BC%3AbestSeller%3AproductPrimaryCategoryCode%3A1848&show=All\"]\n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define base URL and total pages to scrape for Migros\n",
    "        migros_base_url = \"https://www.migros.com.tr/arama?q=bebek%20s%C3%BCt%C3%BC&kategori=70507&sirala=akilli-siralama&sayfa=\"\n",
    "        migros_total_pages = 2\n",
    "\n",
    "        migros_data = scrape_migros_products(migros_base_url, migros_total_pages)\n",
    "\n",
    "\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        \n",
    "\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "        driver.quit()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df = product_df.copy()\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Bebek Sütü (Toz Karışım)\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Bebek Sütü (Toz Karışım)\",data,urunler_df)\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # Function to scrape product names and prices from Migros\n",
    "        def scrape_migros_products(base_url, total_pages):\n",
    "            product_data = []\n",
    "            \n",
    "            for page in range(1, total_pages + 1):\n",
    "                url = f\"{base_url}{page}\"\n",
    "                print(f\"Scraping page {page}: {url}\")\n",
    "                driver.get(url)\n",
    "\n",
    "                try:\n",
    "                    # Wait for the product names to load\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '.product-name'))\n",
    "                    )\n",
    "\n",
    "                    # Get product names and prices\n",
    "                    products = driver.find_elements(By.CSS_SELECTOR, '.product-name')\n",
    "                    prices = driver.find_elements(By.CSS_SELECTOR, '.price-new')\n",
    "\n",
    "                    # Ensure we have matching names and prices\n",
    "                    if len(products) != len(prices):\n",
    "                        print(f\"Warning: Number of products and prices do not match on page {page}\")\n",
    "\n",
    "                    for i in range(len(products)):\n",
    "                        try:\n",
    "                            product_name = products[i].text.strip()\n",
    "                            product_price_text = prices[i].text.strip()\n",
    "                            product_price = clean_price(product_price_text)  # Convert to float\n",
    "                            product_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "                            print(f\"Product: {product_name}, Price: {product_price} TRY\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error scraping product {i} on page : {e}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading products on page : {e}\")\n",
    "\n",
    "                # Sleep to avoid being detected as a bot\n",
    "                sleep(2)\n",
    "            \n",
    "            return product_data\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour\n",
    "        def scrape_carrefour_products():\n",
    "            carrefour_data=[]\n",
    "            pages = [\"https://www.carrefoursa.com/bulgur/c/1142?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define base URL and total pages to scrape for Migros\n",
    "        migros_base_url = \"https://www.migros.com.tr/arama?q=bulgur&kategori=1062&sirala=akilli-siralama&sayfa=\"\n",
    "        migros_total_pages = 2\n",
    "\n",
    "        # Scrape Migros products\n",
    "        migros_data = scrape_migros_products(migros_base_url, migros_total_pages)\n",
    "\n",
    "        # Scrape Carrefour products\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        all_data = migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "                urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "                urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "                urunler_df.index=len(urunler_df)*[\"Bulgur\"]\n",
    "                urunler_df=urunler_df.drop_duplicates()\n",
    "                urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "                data=veriekle(\"Bulgur\",data,urunler_df)\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # Function to scrape product names and prices from Migros\n",
    "        def scrape_migros_products(base_url, total_pages):\n",
    "            product_data = []\n",
    "            \n",
    "            for page in range(1, total_pages + 1):\n",
    "                url = f\"{base_url}{page}\"\n",
    "                print(f\"Scraping page {page}: {url}\")\n",
    "                driver.get(url)\n",
    "\n",
    "                try:\n",
    "                    # Wait for the product names to load\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '.product-name'))\n",
    "                    )\n",
    "\n",
    "                    # Get product names and prices\n",
    "                    products = driver.find_elements(By.CSS_SELECTOR, '.product-name')\n",
    "                    prices = driver.find_elements(By.CSS_SELECTOR, '.price-new')\n",
    "\n",
    "                    # Ensure we have matching names and prices\n",
    "                    if len(products) != len(prices):\n",
    "                        print(f\"Warning: Number of products and prices do not match on page {page}\")\n",
    "\n",
    "                    for i in range(len(products)):\n",
    "                        try:\n",
    "                            product_name = products[i].text.strip()\n",
    "                            product_price_text = prices[i].text.strip()\n",
    "                            product_price = clean_price(product_price_text)  # Convert to float\n",
    "                            product_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "                            print(f\"Product: {product_name}, Price: {product_price} TRY\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error scraping product {i} on page : {e}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading products on page : {e}\")\n",
    "\n",
    "                # Sleep to avoid being detected as a bot\n",
    "                sleep(2)\n",
    "            \n",
    "            return product_data\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour\n",
    "        def scrape_carrefour_products():\n",
    "            carrefour_data=[]\n",
    "            pages = [\"https://www.carrefoursa.com/search?q=ekmek%3AbestSeller%3AinStockFlag%3Atrue%3AproductPrimaryCategoryCode%3A1398\",\"https://www.carrefoursa.com/search?q=ekmek%3AbestSeller%3AinStockFlag%3Atrue%3AproductPrimaryCategoryCode%3A1401\"]\n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define base URL and total pages to scrape for Migros\n",
    "        migros_base_url = \"https://www.migros.com.tr/arama?q=ekmek&kategori=1109&markalar=492&sirala=akilli-siralama&sayfa=\"\n",
    "        migros_total_pages = 1\n",
    "\n",
    "        # Scrape Migros products\n",
    "        migros_data = scrape_migros_products(migros_base_url, migros_total_pages)\n",
    "\n",
    "        # Scrape Carrefour products\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Ekmek\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Ekmek\",data,urunler_df)\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        def scrape_migros_products(base_url, total_pages):\n",
    "            product_data = []\n",
    "            \n",
    "            for page in range(1, total_pages + 1):\n",
    "                url = f\"{base_url}{page}\"\n",
    "                print(f\"Scraping page {page}: {url}\")\n",
    "                driver.get(url)\n",
    "\n",
    "                try:\n",
    "                    # Wait for the product names to load\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '.product-name'))\n",
    "                    )\n",
    "\n",
    "                    # Get all product cards\n",
    "                    product_cards = driver.find_elements(By.CSS_SELECTOR, 'sm-list-page-item')\n",
    "\n",
    "                    for product_card in product_cards:\n",
    "                        # Check if the product is sponsored by looking for a specific sponsored class or attribute\n",
    "                        if \"external-list-item\" in product_card.get_attribute('class').lower():\n",
    "                            print(\"Skipping sponsored product.\")\n",
    "                            continue  # Skip this sponsored product\n",
    "\n",
    "                        try:\n",
    "                            # Extract the product name\n",
    "                            product_name = product_card.find_element(By.CSS_SELECTOR, '.product-name').text.strip()\n",
    "\n",
    "                            # Try to extract the price from multiple possible locations\n",
    "                            price_element = None\n",
    "                            try:\n",
    "                                # First attempt using the primary price selector\n",
    "                                price_element = product_card.find_element(By.CSS_SELECTOR, '.price-new')\n",
    "                            except Exception:\n",
    "                                try:\n",
    "                                    # If not found, try another potential selector for the price\n",
    "                                    price_element = product_card.find_element(By.CSS_SELECTOR, '.price .amount')\n",
    "                                except Exception:\n",
    "                                    print(f\"Price not found for product: {product_name}\")\n",
    "                                    continue  # Skip if no price is found\n",
    "\n",
    "                            if price_element:\n",
    "                                product_price_text = price_element.text.strip()\n",
    "                                product_price = clean_price(product_price_text)  # Convert price to float\n",
    "                                # Append product name and price to the list\n",
    "                                product_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "                                print(f\"Product: {product_name}, Price: {product_price} TRY\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error scraping a product on page: {e}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading products on page : {e}\")\n",
    "\n",
    "                # Sleep to avoid being detected as a bot\n",
    "                sleep(2)\n",
    "            \n",
    "            return product_data\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour\n",
    "        def scrape_carrefour_products():\n",
    "                carrefour_data=[]\n",
    "                url = \"https://www.carrefoursa.com/search?q=bisk%C3%BCvi%3AbestSeller%3AproductPrimaryCategoryCode%3A1529%3AinStockFlag%3Atrue&show=All\"\n",
    "\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "                return carrefour_data\n",
    "\n",
    "        # Define base URL and total pages to scrape for Migros\n",
    "        migros_base_url = \"https://www.migros.com.tr/arama?q=bisk%C3%BCvi&kategori=1084&sayfa=\"\n",
    "        migros_total_pages = 8\n",
    "\n",
    "        # Scrape Migros products\n",
    "        migros_data = scrape_migros_products(migros_base_url, migros_total_pages)\n",
    "\n",
    "        # Scrape Carrefour products\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "        driver.quit()\n",
    "        \n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df[urunler_df['Ürün'].str.contains(\"Bisküvi\", case=False)]\n",
    "\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Bisküvi\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Bisküvi\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # Function to scrape product names and prices from Migros for multiple URLs\n",
    "        def scrape_migros_products(pages):\n",
    "            product_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "                driver.get(url)\n",
    "\n",
    "                try:\n",
    "                    # Wait for the product names to load\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '.product-name'))\n",
    "                    )\n",
    "\n",
    "                    # Get all product cards\n",
    "                    product_cards = driver.find_elements(By.CSS_SELECTOR, 'sm-list-page-item')\n",
    "\n",
    "                    for product_card in product_cards:\n",
    "                        # Check if the product is sponsored by looking for a specific sponsored class or attribute\n",
    "                        if \"external-list-item\" in product_card.get_attribute('class').lower():\n",
    "                            print(\"Skipping sponsored product.\")\n",
    "                            continue  # Skip this sponsored product\n",
    "\n",
    "                        try:\n",
    "                            # Extract the product name\n",
    "                            product_name = product_card.find_element(By.CSS_SELECTOR, '.product-name').text.strip()\n",
    "\n",
    "                            # Try to extract the price from multiple possible locations\n",
    "                            price_element = None\n",
    "                            try:\n",
    "                                # First attempt using the primary price selector\n",
    "                                price_element = product_card.find_element(By.CSS_SELECTOR, '.price-new')\n",
    "                            except Exception:\n",
    "                                try:\n",
    "                                    # If not found, try another potential selector for the price\n",
    "                                    price_element = product_card.find_element(By.CSS_SELECTOR, '.price .amount')\n",
    "                                except Exception:\n",
    "                                    print(f\"Price not found for product: {product_name}\")\n",
    "                                    continue  # Skip if no price is found\n",
    "\n",
    "                            if price_element:\n",
    "                                product_price_text = price_element.text.strip()\n",
    "                                product_price = clean_price(product_price_text)  # Convert price to float\n",
    "                                # Append product name and price to the list\n",
    "                                product_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "                                print(f\"Product: {product_name}, Price: {product_price} TRY\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error scraping a product on page: {e}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading products on page: {e}\")\n",
    "\n",
    "                # Sleep to avoid being detected as a bot\n",
    "                sleep(2)\n",
    "            \n",
    "            return product_data\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/search?q=kraker%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/arama?q=kraker&sayfa=1&kategori=10218&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=kraker&sayfa=2&kategori=10218&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=kraker&sayfa=3&kategori=10218&sirala=akilli-siralama\"]\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Kraker\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Kraker\",data,urunler_df)\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # Function to scrape product names and prices from Migros for multiple URLs\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/search?q=gofret%3AbestSeller%3AinStockFlag%3Atrue&text=gofret#\"]\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/arama?q=gofret&sayfa=1&kategori=1082&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=gofret&sayfa=2&kategori=1082&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=gofret&sayfa=3&kategori=1082&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=gofret&sayfa=4&kategori=1082&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=gofret&sayfa=5&kategori=1082&sirala=akilli-siralama\"]\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Gofret\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            data=veriekle(\"Gofret\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/pastalar/c/1289?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/arama?q=pasta&sayfa=1&kategori=1113\",\n",
    "                        \"https://www.migros.com.tr/arama?q=pasta&sayfa=1&kategori=1111\"]\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Pasta\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Pasta\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/search?q=kek%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/arama?q=kek&sayfa=1&kategori=1085\"]\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Kek\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Kek\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/search?q=baklava%3AbestSeller%3AinStockFlag%3Atrue%3AproductPrimaryCategoryCode%3A1294\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/arama?q=baklava&sayfa=1&kategori=126\"\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Baklava\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Baklava\",data,urunler_df)\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/baliklar/c/1099?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/arama?q=yufka\"\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        all_data = migros_data \n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Ekmek Hamuru (Yufka)\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Ekmek Hamuru (Yufka)\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/makarna/c/1122?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/arama?q=makarna&sayfa=1&kategori=10112&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=makarna&sayfa=2&kategori=10112&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=makarna&sayfa=3&kategori=10112&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=makarna&sayfa=4&kategori=10112&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=makarna&sayfa=5&kategori=10112&sirala=akilli-siralama\"\n",
    "                        \n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Şehriye\", regex=True)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Makarna\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Makarna\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # Function to scrape product names and prices from Migros for multiple URLs\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/search?q=%C5%9Fehriye%3AbestSeller%3AinStockFlag%3Atrue%3AproductPrimaryCategoryCode%3A1122\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/arama?q=%C5%9Fehriye&sayfa=1&kategori=5\"\n",
    "        ]\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Makarna\", regex=True)]\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Bulgur\", regex=True)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Şehriye\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            data=veriekle(\"Şehriye\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # Function to scrape product names and prices from Migros for multiple URLs\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/musli-hububat-urunleri/c/1378?q=%3AbestSeller%3Acategory%3A1310%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/kahvaltilik-gevrek-c-422?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/kahvaltilik-gevrek-c-422?sayfa=2&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/kahvaltilik-gevrek-c-422?sayfa=3&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/kahvaltilik-gevrek-c-422?sayfa=4&sirala=onerilenler\"\n",
    "                        \n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Tahıl Gevreği\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Tahıl Gevreği\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/search?q=dana+eti%3AbestSeller%3AinStockFlag%3Atrue%3AproductPrimaryCategoryCode%3A1046\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/dana-eti-c-3fa?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/dana-eti-c-3fa?sayfa=2&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/dana-eti-c-3fa?sayfa=3&sirala=onerilenler\",\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Dana Eti\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Dana Eti\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/kuzu/c/1054?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/kuzu-eti-c-3fb?sayfa=1\"\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Kuzu Eti\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Kuzu Eti\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/pilic/c/1061?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/pilic-c-3fe?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/pilic-c-3fe?sayfa=2&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/pilic-c-3fe?sayfa=3&sirala=onerilenler\",\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Tavuk Eti\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Tavuk Eti\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/sakatat-c-3fd\"\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        all_data = migros_data \n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Sakatat\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Sakatat\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/sucuk/c/1077?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/sucuk-c-404?sayfa=2&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/sucuk-c-404?sayfa=1&sirala=onerilenler\",\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Sucuk\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Sucuk\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/sosis/c/1084?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/sosis-c-405?sayfa=1\"\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Salam\", regex=True)]\n",
    "            urunler_df.index=len(urunler_df)*[\"Sosis\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Sosis\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/salam-jambon-ve-fume/c/1092?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/salam-c-112d6?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/salam-c-112d6?sayfa=2&sirala=onerilenler\",\n",
    "                \n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Salam\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Salam\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/pratik-yemek-c-44f?sayfa=1&90=503&sirala=onerilenler\"\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        all_data = migros_data \n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Hazır Et Yemekleri\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Hazır Et Yemekleri\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/baliklar/c/1099?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/mevsim-baliklari-c-402?sayfa=1\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Balık\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Balık\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/paketli-urunler/c/1068?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/dondurulmus-deniz-urunleri-c-2830?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/dondurulmus-deniz-urunleri-c-2830?sayfa=2&sirala=onerilenler\"\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Konserve Balık\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Konserve Balık\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/sut/c/1311?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/sut-c-6c?sayfa=1&109=1020&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/sut-c-6c?sayfa=2&109=1020&sirala=onerilenler\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "            urunler_df.index=len(urunler_df)*[\"Süt\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Süt\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/yogurt/c/1389?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/yogurt-c-6e?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/yogurt-c-6e?sayfa=2&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/yogurt-c-6e?sayfa=3&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/yogurt-c-6e?sayfa=4&sirala=onerilenler\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Yoğurt\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Yoğurt\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/sutlu-tatli-puding/c/1962?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/geleneksel-sutlu-tatlilar-c-2765?sayfa=1&sirala=onerilenler\"\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Hazır Sütlü Tatlılar\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            data=veriekle(\"Hazır Sütlü Tatlılar\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/beyaz-peynir/c/1319?q=%3AbestSeller&show=All\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/inek-peyniri-c-2731?sayfa=1\",\n",
    "                        \"https://www.migros.com.tr/koyun-peyniri-c-2732?sayfa=1\",\n",
    "                        \"https://www.migros.com.tr/suzme-peynir-c-2733?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/suzme-peynir-c-2733?sayfa=2&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/keci-peyniri-c-2735?sayfa=1\"\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Beyaz Peynir\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Beyaz Peynir\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/kasar-/c/1324?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/kasar-peyniri-c-40d?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/kasar-peyniri-c-40d?sayfa=2&sirala=onerilenler\"\n",
    "                \n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Kaşar Peyniri\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Kaşar Peyniri\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/krem-peynir/c/1336?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/arama?q=krem%20peynir&sayfa=1&kategori=10039&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=krem%20peynir&sayfa=2&kategori=10039&sirala=akilli-siralama\"\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Krem Peynir\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            data=veriekle(\"Krem Peynir\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/yumurta/c/1349?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =   [\"https://www.migros.com.tr/yumurta-c-70\"\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Yumurta\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            data=veriekle(\"Yumurta\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/tereyag/c/1350?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/tereyagi-c-413?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/tereyagi-c-413?sayfa=2&sirala=onerilenler\"\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Tereyağı (Kahvaltılık)\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Tereyağı (Kahvaltılık)\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/margarin/c/1351?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/margarin-c-414?sayfa=1\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Margarin\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            data=veriekle(\"Margarin\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/zeytinyagi/c/1114?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/zeytinyagi-c-433?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/zeytinyagi-c-433?sayfa=2&sirala=onerilenler\"\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Zeytinyağı\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Zeytinyağı\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/aycicek/c/1112?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/aycicek-yagi-c-42d?sayfa=1\"\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Ayçiçek Yağı\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            data=veriekle(\"Ayçiçek Yağı\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/search?q=portakal%3AbestSeller%3AproductPrimaryCategoryCode%3A1016%3AinStockFlag%3Atrue&text=portakal#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/narenciye-c-3ec?sayfa=1&93=652&sirala=onerilenler\"\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "\n",
    "        if migros_data and carrefour_data:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "        elif migros_data and not carrefour_data:\n",
    "                    all_data=migros_data\n",
    "        else:\n",
    "                    all_data=carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Portakal\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Portakal\",data,urunler_df)\n",
    "\n",
    "\n",
    "        def vericek(carrefour=\"\",migros=\"\",name=\"\"):\n",
    "            try:  \n",
    "\n",
    "            # Function to clean and convert price text to float\n",
    "                def clean_price(price_text):\n",
    "                    # Remove any non-numeric characters except for commas and dots\n",
    "                    price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "                    # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "                    price_text = price_text.replace(',', '.')\n",
    "                    try:\n",
    "                        return float(price_text)\n",
    "                    except ValueError:\n",
    "                        return None\n",
    "\n",
    "\n",
    "\n",
    "                # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "                def scrape_carrefour_products(carrefour):\n",
    "                    \n",
    "                    pages = carrefour\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "                    carrefour_data = []\n",
    "                    \n",
    "                    for url in pages:\n",
    "                    \n",
    "                            # Send a GET request to fetch the HTML content\n",
    "                        response = requests.get(url)\n",
    "\n",
    "                        # Check if the request was successful\n",
    "                        if response.status_code == 200:\n",
    "                            # Parse the HTML content\n",
    "                            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                            \n",
    "                            # Find all product cards\n",
    "                            product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                            \n",
    "                            # Loop through each product card and extract the product name and price\n",
    "                            for index, product_card in enumerate(product_cards):\n",
    "                                # Extract product name\n",
    "                                product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                                \n",
    "                                # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                                product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                                \n",
    "                                # Convert price to float\n",
    "                                try:\n",
    "                                    product_price = float(product_price)\n",
    "                                except ValueError:\n",
    "                                    product_price = None  # Handle cases where the price might not be convertible\n",
    "                                \n",
    "                                # Print product name and price\n",
    "                                print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                                carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                        else:\n",
    "                            print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "                    \n",
    "                    return carrefour_data\n",
    "\n",
    "                # Define the list of pages to scrape from Migros\n",
    "                migros_pages  = migros\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                if migros:\n",
    "                    migros_data = scrape_migros_products(migros)\n",
    "                if carrefour:\n",
    "                    carrefour_data = scrape_carrefour_products(carrefour)\n",
    "\n",
    "                if migros and carrefour:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "                elif migros and not carrefour:\n",
    "                    all_data=migros_data\n",
    "                else:\n",
    "                    all_data=carrefour_data\n",
    "                product_df = pd.DataFrame(all_data)\n",
    "\n",
    "                # Close the browser\n",
    "                driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                urunler_df = product_df.copy()\n",
    "                if urunler_df is not None:\n",
    "                    urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "                    urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "                    urunler_df.index=len(urunler_df)*[name]\n",
    "                    urunler_df=urunler_df.drop_duplicates()\n",
    "                    urunler_df=urunler_df.dropna()\n",
    "\n",
    "                    return urunler_df\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "                \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=%C3%BCz%C3%BCm%3AbestSeller%3AproductPrimaryCategoryCode%3A1017%3AinStockFlag%3Atrue&text=%C3%BCz%C3%BCm#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=%C3%BCz%C3%BCm&sayfa=1&kategori=101\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Üzüm\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Üzüm\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=armut%3AbestSeller%3AproductPrimaryCategoryCode%3A1018%3AinStockFlag%3Atrue&text=armut#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=armut&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Armut\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Armut\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=ayva%3AbestSeller%3AproductPrimaryCategoryCode%3A1018%3AinStockFlag%3Atrue&text=ayva#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=ayva&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Ayva\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Ayva\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=%C3%A7ilek%3AbestSeller%3AproductPrimaryCategoryCode%3A1017%3AinStockFlag%3Atrue&text=%C3%A7ilek#\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=carrefour,name=\"Çilek\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Çilek\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=elma%3AbestSeller%3AproductPrimaryCategoryCode%3A1018%3AinStockFlag%3Atrue&text=elma#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=elma&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Elma\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Elma\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"carrefour=[\"https://www.carrefoursa.com/search/?q=karpuz%3AbestSeller%3AinStockFlag%3Atrue&text=karpuz#\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=carrefour,name=\"Karpuz\")\n",
    "        urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Karpuz kg|Karpuz Kg\")]\n",
    "\n",
    "\n",
    "        data=veriekle(\"Karpuz\",data,urunler_df)\"\"\"\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=kavun%3AbestSeller%3AinStockFlag%3Atrue%3AproductPrimaryCategoryCode%3A1018\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=kavun&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kavun\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Kavun\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=kivi%3AbestSeller%3AinStockFlag%3Atrue&text=kivi#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=kivi&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kivi\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"ml\")]\n",
    "\n",
    "\n",
    "        data=veriekle(\"Kivi\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=limon%3AbestSeller%3AproductPrimaryCategoryCode%3A1016%3AinStockFlag%3Atrue&text=limon#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=limon&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Limon\")\n",
    "\n",
    "        data=veriekle(\"Limon\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=mandalina%3AbestSeller%3AinStockFlag%3Atrue&text=mandalina#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=mandalina&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Mandalina\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"kg|Kg\")]\n",
    "\n",
    "\n",
    "\n",
    "        data=veriekle(\"Mandalina\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=muz%3AbestSeller%3AproductPrimaryCategoryCode%3A1022%3AinStockFlag%3Atrue&text=muz#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=muz&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Muz\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Muz\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=%C5%9Feftali%3AbestSeller%3AproductPrimaryCategoryCode%3A1018%3AinStockFlag%3Atrue&text=%C5%9Feftali#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=%C5%9Feftali&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Şeftali\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Şeftali\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=nar%3AbestSeller%3AproductPrimaryCategoryCode%3A1018%3AinStockFlag%3Atrue&text=nar#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=nar&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Nar\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Nar\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=badem%3AbestSeller%3AproductPrimaryCategoryCode%3A1519%3AinStockFlag%3Atrue&text=badem#\"]\n",
    "        migros=[\"https://www.migros.com.tr/badem-c-280f\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Badem İçi\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Badem İçi\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=ceviz%3AbestSeller%3AproductPrimaryCategoryCode%3A1519%3AinStockFlag%3Atrue&text=ceviz#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=ceviz&sayfa=1&kategori=1089\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Ceviz İçi\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Ceviz İçi\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=f%C4%B1nd%C4%B1k%3AbestSeller%3AproductPrimaryCategoryCode%3A1519%3AinStockFlag%3Atrue&text=f%C4%B1nd%C4%B1k#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=f%C4%B1nd%C4%B1k&sayfa=1&kategori=1090\",\"https://www.migros.com.tr/arama?q=f%C4%B1nd%C4%B1k&sayfa=1&kategori=1089\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Fındık İçi\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Fındık İçi\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=Antep%20F%C4%B1st%C4%B1%C4%9F%C4%B1&sayfa=1&kategori=1090\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=\"\",migros=migros,name=\"Antep Fıstığı\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Antep Fıstığı\",data,urunler_df)\n",
    "\n",
    "\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=Yer%20F%C4%B1st%C4%B1%C4%9F%C4%B1&sayfa=1&kategori=1090\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=\"\",migros=migros,name=\"Yer Fıstığı\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Yer Fıstığı\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=leblebi%3AbestSeller%3AinStockFlag%3Atrue&text=leblebi#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=leblebi&sayfa=1&kategori=70651\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Leblebi\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Leblebi\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=ay%C3%A7ekirde%C4%9Fi%3AbestSeller%3AinStockFlag%3Atrue&text=ay%C3%A7ekirde%C4%9Fi#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=ay%20%C3%A7ekirde%C4%9Fi\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Ay Çekirdeği\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Ayçekirdeği|Ayçekirdek|Şimşek\")]\n",
    "\n",
    "\n",
    "\n",
    "        data=veriekle(\"Ay Çekirdeği\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=kabak+%C3%A7ekirde%C4%9Fi%3AbestSeller%3AinStockFlag%3Atrue&text=kabak+%C3%A7ekirde%C4%9Fi#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=kabak%20%C3%A7ekirde%C4%9Fi&sayfa=1&kategori=70651\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kabak Çekirdeği\")\n",
    "        \n",
    "\n",
    "        data=veriekle(\"Kabak Çekirdeği\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=Kuru+%C3%9Cz%C3%BCm%3AbestSeller%3AinStockFlag%3Atrue%3AproductPrimaryCategoryCode%3A1519\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=Kuru%20%C3%9Cz%C3%BCm\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kuru Üzüm\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"üzüm|Üzüm\")]\n",
    "\n",
    "        data=veriekle(\"Kuru Üzüm\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=kuru+kay%C4%B1s%C4%B1%3AbestSeller%3AproductPrimaryCategoryCode%3A1519%3AinStockFlag%3Atrue&text=kuru+kay%C4%B1s%C4%B1#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=kuru%20kay%C4%B1s%C4%B1\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kuru Kayısı\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"kayısı|Kayısı\")]\n",
    "        \n",
    "\n",
    "\n",
    "        data=veriekle(\"Kuru Kayısı\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=%C3%A7arliston%3AbestSeller%3AproductPrimaryCategoryCode%3A1027%3AinStockFlag%3Atrue&text=%C3%A7arliston#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=%C3%A7arliston&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Çarliston Biber\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Çarliston|çarliston\")]\n",
    "\n",
    "        data=veriekle(\"Çarliston Biber\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=Dolmal%C4%B1k+Biber%3AbestSeller%3AproductPrimaryCategoryCode%3A1027\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=Dolmal%C4%B1k%20Biber&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Dolmalık Biber\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Dolma|Dolmalık\")]\n",
    "\n",
    "\n",
    "        data=veriekle(\"Dolmalık Biber\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=Sivri+Biber%3AbestSeller%3AproductPrimaryCategoryCode%3A1027%3AinStockFlag%3Atrue&text=Sivri+Biber#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=Sivri%20Biber\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Sivri Biber\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"sivri|Sivri\")]\n",
    "\n",
    "        data=veriekle(\"Sivri Biber\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=dereotu%3AbestSeller%3AinStockFlag%3Atrue&text=dereotu#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=dereotu\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Dereotu\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Dereotu\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=domates%3AbestSeller%3AinStockFlag%3Atrue&text=domates#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=domates&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Domates\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Salçası\", regex=True)]\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"kg\", regex=True)]\n",
    "\n",
    "            data=veriekle(\"Domates\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=taze+fasulye%3AbestSeller%3AinStockFlag%3Atrue%3AproductPrimaryCategoryCode%3A1031\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=taze%20fasulye&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Taze Fasulye\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Taze Fasulye\",data,urunler_df)\n",
    "\n",
    "\n",
    "        def scrape_migros_products(pages):\n",
    "            product_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "                driver.get(url)\n",
    "\n",
    "                try:\n",
    "                    # Wait for the product names to load\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '.product-name'))\n",
    "                    )\n",
    "\n",
    "                    # Get all product cards\n",
    "                    product_cards = driver.find_elements(By.CSS_SELECTOR, 'sm-list-page-item')\n",
    "\n",
    "                    for product_card in product_cards:\n",
    "                        # Check if the product is sponsored by looking for a specific sponsored class or attribute\n",
    "                        if \"external-list-item\" in product_card.get_attribute('class').lower():\n",
    "                            print(\"Skipping sponsored product.\")\n",
    "                            continue  # Skip this sponsored product\n",
    "\n",
    "                        try:\n",
    "                            # Extract the product name\n",
    "                            product_name = product_card.find_element(By.CSS_SELECTOR, '.product-name').text.strip()\n",
    "\n",
    "                            # Try to extract the price from multiple possible locations\n",
    "                            price_element = None\n",
    "                            try:\n",
    "                                # First attempt using the primary price selector\n",
    "                                price_element = product_card.find_element(By.CSS_SELECTOR, '.price-new')\n",
    "                            except Exception:\n",
    "                                try:\n",
    "                                    # If not found, try another potential selector for the price\n",
    "                                    price_element = product_card.find_element(By.CSS_SELECTOR, '.price .amount')\n",
    "                                except Exception:\n",
    "                                    print(f\"Price not found for product: {product_name}\")\n",
    "                                    continue  # Skip if no price is found\n",
    "\n",
    "                            if price_element:\n",
    "                                product_price_text = price_element.text.strip()\n",
    "                                product_price = clean_price(product_price_text)  # Convert price to float\n",
    "                                # Append product name and price to the list\n",
    "                                product_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "                                print(f\"Product: {product_name}, Price: {product_price} TRY\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error scraping a product on page: {e}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading products on page: {e}\")\n",
    "\n",
    "                # Sleep to avoid being detected as a bot\n",
    "                sleep(2)\n",
    "            \n",
    "            return product_data\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=havu%C3%A7%3AbestSeller%3Acategory%3A1014\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=havu%C3%A7&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Havuç\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Havuç\",data,urunler_df)\n",
    "\n",
    "\n",
    "        try:\n",
    "            carrefour=[\"https://www.carrefoursa.com/search?q=%C4%B1spanak%3AbestSeller%3AproductPrimaryCategoryCode%3A1030%3AinStockFlag%3Atrue&text=%C4%B1spanak#\"]\n",
    "            migros=[\"https://www.migros.com.tr/arama?q=%C4%B1spanak&sayfa=1&kategori=2&markalar=492&sirala=akilli-siralama\"]\n",
    "            options = Options()\n",
    "            options.headless = False \n",
    "\n",
    "\n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            driver = webdriver.Chrome(service=service, options=options)\n",
    "            urunler_df=vericek(carrefour,migros,\"Ispanak\")\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Ispanak\")]\n",
    "\n",
    "            data=veriekle(\"Ispanak\",data,urunler_df)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=kabak%3AbestSeller%3AinStockFlag%3Atrue&text=kabak#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=kabak\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kabak\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Sakız|Dolmalık|Adet\")]\n",
    "\n",
    "        data=veriekle(\"Kabak\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=karnabahar%3AbestSeller%3Acategory%3A1014%3AinStockFlag%3Atrue&text=karnabahar#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=karnabahar&sayfa=1&markalar=492&sirala=akilli-siralama\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Karnabahar\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Karnabahar\")]\n",
    "\n",
    "\n",
    "        data=veriekle(\"Karnabahar\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=kuru+so%C4%9Fan%3AbestSeller%3AproductPrimaryCategoryCode%3A1033%3AinStockFlag%3Atrue&text=kuru+so%C4%9Fan#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=kuru%20so%C4%9Fan\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kuru Soğan\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"soğan|Soğan\")]\n",
    "\n",
    "        data=veriekle(\"Kuru Soğan\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=beyaz%20lahana&sayfa=1&markalar=492&sirala=akilli-siralama\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=\"\",migros=migros,name=\"Beyaz Lahana\")\n",
    "\n",
    "\n",
    "\n",
    "        data=veriekle(\"Beyaz Lahana\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=k%C4%B1rm%C4%B1z%C4%B1%20lahana&sayfa=1&markalar=492&sirala=akilli-siralama\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=\"\",migros=migros,name=\"Kırmızı Lahana\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Kırmızı Lahana\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=mantar%3AbestSeller%3AinStockFlag%3Atrue&text=mantar#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=mantar&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Mantar\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Mantar|mantar|mantarı|Mantarı\")]\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Çorba|Pano|İstiridye|Salatası\")]\n",
    "\n",
    "        data=veriekle(\"Mantar\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=k%C4%B1v%C4%B1rc%C4%B1k%3AbestSeller%3AinStockFlag%3Atrue&text=k%C4%B1v%C4%B1rc%C4%B1k#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=k%C4%B1v%C4%B1rc%C4%B1k&sayfa=1&markalar=492&sirala=akilli-siralama\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kıvırcık\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"kıvırcık|Kıvırcık\")]\n",
    "        data=veriekle(\"Kıvırcık\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=maydonoz%3AbestSeller%3AinStockFlag%3Atrue&text=maydonoz#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=maydonoz&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Maydanoz\")\n",
    "\n",
    "        data=veriekle(\"Maydanoz\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=nane%3AbestSeller%3AproductPrimaryCategoryCode%3A1030%3AinStockFlag%3Atrue&text=nane#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=nane&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Nane\")\n",
    "\n",
    "        data=veriekle(\"Nane\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=patl%C4%B1can%3AbestSeller%3AproductPrimaryCategoryCode%3A1031%3AinStockFlag%3Atrue&text=patl%C4%B1can#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=patl%C4%B1can&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Patlıcan\")\n",
    "\n",
    "        data=veriekle(\"Patlıcan\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=p%C4%B1rasa%3AbestSeller%3AinStockFlag%3Atrue&text=p%C4%B1rasa#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=p%C4%B1rasa\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Pırasa\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"pırasa|Pırasa\")]\n",
    "\n",
    "        data=veriekle(\"Pırasa\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=roka%3AbestSeller%3AinStockFlag%3Atrue&text=roka#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=roka&sayfa=1&markalar=492&sirala=akilli-siralama\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Roka\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"roka|Roka\")]\n",
    "        data=veriekle(\"Roka\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=salatal%C4%B1k%3AbestSeller%3Acategory%3A1014%3AinStockFlag%3Atrue&text=salatal%C4%B1k#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=salatal%C4%B1k&sayfa=1&markalar=492&sirala=akilli-siralama&kategori=102\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Salatalık\")\n",
    "\n",
    "        data=veriekle(\"Salatalık\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/kuru-sarimsak-kg-p-30024962\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=sar%C4%B1msak&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Sarımsak\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Sarımsak\", regex=True)]\n",
    "\n",
    "            data=veriekle(\"Sarımsak\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=k%C4%B1rm%C4%B1z%C4%B1%20turp\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=\"\",migros=migros,name=\"Kırmızı Turp\")\n",
    "        data=veriekle(\"Kırmızı Turp\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=patates%3AbestSeller%3AproductPrimaryCategoryCode%3A1033%3AinStockFlag%3Atrue&text=patates#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=patates&sayfa=1&kategori=1014&markalar=492&sirala=akilli-siralama\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Patates\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Sarımsak|Soğan\", regex=True)]\n",
    "            data=veriekle(\"Patates\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=kuru+fasulye%3AbestSeller%3AinStockFlag%3Atrue%3Acategory%3A1110\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=kuru%20fasulye&sayfa=1&kategori=5\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kuru Fasulye\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"fasulye|Fasulye\")]\n",
    "\n",
    "            data=veriekle(\"Kuru Fasulye\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=nohut%3AbestSeller%3AproductPrimaryCategoryCode%3A1152%3AinStockFlag%3Atrue&text=nohut#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=nohut&sayfa=1&kategori=10136\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Nohut\")\n",
    "        data=veriekle(\"Nohut\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=mercimek%3AbestSeller%3AproductPrimaryCategoryCode%3A1152%3AinStockFlag%3Atrue&text=mercimek#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=mercimek&sayfa=1&kategori=70601\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Mercimek\")\n",
    "        data=veriekle(\"Mercimek\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/diger-sebze/c/1193?q=%3AbestSeller&show=All\",\"https://www.carrefoursa.com/yesil-sebze/c/1187?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=konserve&sayfa=1&kategori=10324\",\n",
    "                \"https://www.migros.com.tr/arama?q=konserve&sayfa=1&kategori=10304\",\n",
    "                \"https://www.migros.com.tr/arama?q=konserve&sayfa=1&kategori=10325\",\n",
    "                \"https://www.migros.com.tr/arama?q=konserve&sayfa=1&kategori=10328\",\n",
    "                \"https://www.migros.com.tr/arama?q=konserve&sayfa=1&kategori=10326\",\n",
    "                \"https://www.migros.com.tr/arama?q=konserve&sayfa=1&kategori=10322\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Konserveler\")\n",
    "        data=veriekle(\"Konserveler\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=tur%C5%9Fu%3AbestSeller%3AinStockFlag%3Atrue&text=tur%C5%9Fu#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=tur%C5%9Fu&sayfa=1&kategori=1108&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=tur%C5%9Fu&sayfa=2&kategori=1108&sirala=akilli-siralama\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Turşu\")\n",
    "        data=veriekle(\"Turşu\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/salca/c/1180?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=sal%C3%A7a&sayfa=1&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=sal%C3%A7a&sayfa=2&sirala=akilli-siralama\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Salça\")\n",
    "        data=veriekle(\"Salça\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/zeytin/c/1356?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=zeytin&sayfa=1&kategori=113&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=zeytin&sayfa=2&kategori=113&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=zeytin&sayfa=3&kategori=113&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=zeytin&sayfa=4&kategori=113&sirala=akilli-siralama\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Zeytin\")\n",
    "        data=veriekle(\"Zeytin\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/cipsler/c/1552?q=%3AbestSeller%3Acategory%3A1552%3AinStockFlag%3Atrue&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=cips&sayfa=1&kategori=1088&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=cips&sayfa=2&kategori=1088&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=cips&sayfa=3&kategori=1088&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=cips&sayfa=4&kategori=1088&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=cips&sayfa=5&kategori=1088&sirala=akilli-siralama\",\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Cipsler\")\n",
    "        data=veriekle(\"Cipsler\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=Toz+%C5%9Eeker%3AbestSeller%3AinStockFlag%3Atrue&text=Toz+%C5%9Eeker#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=Toz%20%C5%9Eeker&sayfa=1&kategori=172\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Toz Şeker\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Toz\", regex=True)]\n",
    "            data=veriekle(\"Toz Şeker\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=k%C3%BCp+%C5%9Feker%3AbestSeller%3AinStockFlag%3Atrue&text=k%C3%BCp+%C5%9Feker#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=k%C3%BCp%20%C5%9Feker&sayfa=1&kategori=1347\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kesme Şeker\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Küp\", regex=True)]\n",
    "            data=veriekle(\"Kesme Şeker\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=re%C3%A7el%3AbestSeller%3AinStockFlag%3Atrue&text=re%C3%A7el#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=re%C3%A7el&sayfa=1&kategori=10107&sirala=akilli-siralama\",\"https://www.migros.com.tr/arama?q=re%C3%A7el&sayfa=2&kategori=10107&sirala=akilli-siralama\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Reçel\")\n",
    "        data=veriekle(\"Reçel\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/ballar/c/1362?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=bal&sayfa=1&kategori=1056&sirala=akilli-siralama\",\"https://www.migros.com.tr/arama?q=bal&sayfa=2&kategori=1056&sirala=akilli-siralama\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Bal\")\n",
    "        data=veriekle(\"Bal\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=pekmez%3AbestSeller&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=pekmez&sayfa=1&kategori=10096\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Pekmez\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Pekmez|Pekmezi\", regex=True)]\n",
    "            data=veriekle(\"Pekmez\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=tahin+helva%3AbestSeller%3AproductPrimaryCategoryCode%3A1374%3AinStockFlag%3Atrue&text=tahin+helva#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=tahin%20helvas%C4%B1&sayfa=1&kategori=10097\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Tahin Helvası\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Helvası|Helva\", regex=True)]\n",
    "            data=veriekle(\"Tahin Helvası\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=f%C4%B1nd%C4%B1k+ezmesi%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=f%C4%B1nd%C4%B1k%20ezmesi&sayfa=1&kategori=10104\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Fındık Ezmesi\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Fındık Ezmesi\", regex=True)]\n",
    "            data=veriekle(\"Fındık Ezmesi\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=tablet+%C3%A7ikolata%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=tablet%20%C3%A7ikolata\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Çikolata Tablet\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Tablet|Kare\", regex=True)]\n",
    "            data=veriekle(\"Çikolata Tablet\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=krem+%C3%A7ikolata%3AbestSeller%3AproductPrimaryCategoryCode%3A1381%3AinStockFlag%3Atrue&text=krem+%C3%A7ikolata#\"]\n",
    "        migros=[\"https://www.migros.com.tr/kakao-findik-kremalari-c-2779?sayfa=1\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Çikolata Krem\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Ezmesi\", regex=True)]\n",
    "            data=veriekle(\"Çikolata Krem\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=lokum%3AbestSeller%3AproductPrimaryCategoryCode%3A1494%3AinStockFlag%3Atrue&text=lokum#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=lokum&sayfa=1&kategori=10268&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=lokum&sayfa=2&kategori=10268&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=lokum&sayfa=3&kategori=10268&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=lokum&sayfa=4&kategori=10268&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=lokum&sayfa=5&kategori=10268&sirala=akilli-siralama\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Lokum\")\n",
    "        data=veriekle(\"Lokum\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=sak%C4%B1z%3AbestSeller%3AproductPrimaryCategoryCode%3A1501%3AinStockFlag%3Atrue&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=sak%C4%B1z&sayfa=1&kategori=1091&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=sak%C4%B1z&sayfa=2&kategori=1091&sirala=akilli-siralama\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Sakız\")\n",
    "        data=veriekle(\"Sakız\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/sekerleme/c/1494?q=%3AbestSeller%3Acategory%3ABRN-1949%3Acategory%3ABRN-2504%3Acategory%3ABRN-3185%3Acategory%3ABRN-2125%3Acategory%3ABRN-3091%3Acategory%3ABRN-2999%3AinStockFlag%3Atrue&text=#\"]\n",
    "        migros=[\"https://www.migros.com.tr/yumusak-seker-c-2818?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/yumusak-seker-c-2818?sayfa=2&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/yumusak-seker-c-2818?sayfa=3&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/draje-sekerleme-c-2816?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/draje-sekerleme-c-2816?sayfa=2&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kağıtlı Şeker\")\n",
    "        data=veriekle(\"Kağıtlı Şeker\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/kap-dondurma/c/1261?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\",\n",
    "                \"https://www.carrefoursa.com/tek-dondurma/c/1266?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/dondurma-c-41b?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/dondurma-c-41b?sayfa=2&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/dondurma-c-41b?sayfa=3&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/dondurma-c-41b?sayfa=4&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/dondurma-c-41b?sayfa=5&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/dondurma-c-41b?sayfa=6&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Dondurma\")\n",
    "        data=veriekle(\"Dondurma\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/baharat/c/1167?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=baharat&sayfa=1&kategori=10180&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=baharat&sayfa=2&kategori=10180&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=baharat&sayfa=3&kategori=10180&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=baharat&sayfa=4&kategori=10180&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=baharat&sayfa=5&kategori=10180&sirala=akilli-siralama\",\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Baharat\")\n",
    "        data=veriekle(\"Baharat\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/tuz/c/1166?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "        migros=[\"https://www.migros.com.tr/tuz-c-436?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/tuz-c-436?sayfa=2&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Tuz\")\n",
    "        data=veriekle(\"Tuz\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=kabartma%3AbestSeller%3AproductPrimaryCategoryCode%3A1282%3AinStockFlag%3Atrue&text=kabartma#\"]\n",
    "        migros=[\"https://www.migros.com.tr/kabartma-tozu-sekerli-vanilin-c-2893\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kabartma Maddeleri\")\n",
    "        data=veriekle(\"Kabartma Maddeleri\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=sirke%3AbestSeller%3AproductPrimaryCategoryCode%3A1219%3AinStockFlag%3Atrue&text=sirke#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=sirke&sayfa=1&kategori=10319&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=sirke&sayfa=2&kategori=10319&sirala=akilli-siralama\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Sirke\")\n",
    "        data=veriekle(\"Sirke\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=ket%C3%A7ap%3AbestSeller%3AinStockFlag%3Atrue&text=ket%C3%A7ap#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=ket%C3%A7ap&sayfa=1&kategori=10311\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Ketçap\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Mayonez\", regex=True)]\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Ketçap\")]\n",
    "\n",
    "            data=veriekle(\"Ketçap\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=mayonez%3AbestSeller%3AproductPrimaryCategoryCode%3A1212%3AinStockFlag%3Atrue&text=mayonez#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=mayonez&sayfa=1&kategori=10312\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Mayonez\")\n",
    "        data=veriekle(\"Mayonez\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/tahin-pekmez-helva/c/1374?q=%3AbestSeller%3Acategory%3A1310%3AinStockFlag%3Atrue&text=#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=tahin&sayfa=1&kategori=10095\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Tahin\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Tahin\", regex=True)]\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Pekmezi|Helva\", regex=True)]\n",
    "            data=veriekle(\"Tahin\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/hazir-corbalar/c/1224?q=%3AbestSeller&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=haz%C4%B1r%20%C3%A7orba&sayfa=1&kategori=1103&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=haz%C4%B1r%20%C3%A7orba&sayfa=2&kategori=1103&sirala=akilli-siralama\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Hazır Çorbalar\")\n",
    "        data=veriekle(\"Hazır Çorbalar\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/hazirlanacak-tatlilar/c/1300?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/toz-tatlilar-c-287d?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/toz-tatlilar-c-287d?sayfa=2&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/toz-tatlilar-c-287d?sayfa=3&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Hazır Pakette Toz Tatlılar (Puding)\")\n",
    "        data=veriekle(\"Hazır Pakette Toz Tatlılar (Puding)\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=t%C3%BCrk+kahvesi%3AbestSeller%3AinStockFlag%3Atrue&text=t%C3%BCrk+kahvesi#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=t%C3%BCrk%20kahvesi&sayfa=1&sirala=akilli-siralama&kategori=10436\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kahve\")\n",
    "        data=veriekle(\"Kahve\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/kahve/c/1467?q=%3AbestSeller%3Acategory%3A1467%3AinStockFlag%3Atrue&text=#\"]\n",
    "        migros=[\"https://www.migros.com.tr/hazir-kahve-c-11222?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/hazir-kahve-c-11222?sayfa=2&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/hazir-kahve-c-11222?sayfa=3&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/hazir-kahve-c-11222?sayfa=4&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Hazır Kahve\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Türk\", regex=True)]\n",
    "            data=veriekle(\"Hazır Kahve\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=%C3%A7ay&sayfa=1&kategori=10433&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=%C3%A7ay&sayfa=2&kategori=10433&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=%C3%A7ay&sayfa=1&kategori=70174\",\n",
    "                \"https://www.migros.com.tr/arama?q=%C3%A7ay&sayfa=1&kategori=70175\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=\"\",migros=migros,name=\"Çay\")\n",
    "        data=veriekle(\"Çay\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        migros=[\"https://www.migros.com.tr/bitki-cayi-c-28c0?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/bitki-cayi-c-28c0?sayfa=2&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/bitki-cayi-c-28c0?sayfa=3&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=\"\",migros=migros,name=\"Bitki ve Meyve Çayı (Poşet)\")\n",
    "        data=veriekle(\"Bitki ve Meyve Çayı (Poşet)\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=kakaolu%20s%C3%BCt&sayfa=1&kategori=108\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=\"\",migros=migros,name=\"Kakaolu Toz İçecekler\")\n",
    "        data=veriekle(\"Kakaolu Toz İçecekler\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/sular/c/1411?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/su-c-84?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/su-c-84?sayfa=2&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Su\")\n",
    "        data=veriekle(\"Su\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/maden-sulari/c/1412?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "        migros=[\"https://www.migros.com.tr/maden-suyu-c-85?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/maden-suyu-c-85?sayfa=2&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/maden-suyu-c-85?sayfa=3&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Maden Suyu ve Sodası\")\n",
    "        data=veriekle(\"Maden Suyu ve Sodası\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=gazoz%3AbestSeller&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/gazoz-c-467?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/gazoz-c-467?sayfa=2&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/gazoz-c-467?sayfa=3&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Gazoz Meyveli\")\n",
    "        data=veriekle(\"Gazoz Meyveli\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/kola/c/1419?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "        migros=[\"https://www.migros.com.tr/kola-c-465?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/kola-c-465?sayfa=2&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kola\")\n",
    "        data=veriekle(\"Kola\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=so%C4%9Fuk+%C3%A7ay%3AbestSeller&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/soguk-cay-c-28be?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/soguk-cay-c-28be?sayfa=2&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/soguk-cay-c-28be?sayfa=3&sirala=onerilenler\",\n",
    "                \n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Soğuk Çay\")\n",
    "        data=veriekle(\"Soğuk Çay\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=ayran%3AbestSeller&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/ayran-c-47a?sayfa=1\",\n",
    "                \n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Ayran\")\n",
    "        data=veriekle(\"Ayran\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=meyve+suyu%3AbestSeller%3AinStockFlag%3Atrue&text=meyve+suyu#\"]\n",
    "        migros=[\"https://www.migros.com.tr/meyve-suyu-c-46c?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/meyve-suyu-c-46c?sayfa=2&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/meyve-suyu-c-46c?sayfa=3&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/meyve-suyu-c-46c?sayfa=4&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/meyve-suyu-c-46c?sayfa=5&sirala=onerilenler\",\n",
    "                \n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Meyve Suyu\")\n",
    "        data=veriekle(\"Meyve Suyu\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=tulum%20peyniri&sayfa=1&kategori=10036\",\n",
    "                \n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=\"\",migros=migros,name=\"Tulum Peyniri\")\n",
    "        data=veriekle(\"Tulum Peyniri\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=kakao%3AbestSeller%3AproductPrimaryCategoryCode%3A1282%3AinStockFlag%3Atrue&text=kakao#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=kakao&sayfa=1&kategori=1118\"\n",
    "                \n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=carrefour,migros=migros,name=\"Kakao\")\n",
    "        data=veriekle(\"Kakao\",data,urunler_df)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # Function to convert numeric columns to float and drop rows where conversion fails\n",
    "        def convert_to_float_and_drop_non_numeric(df):\n",
    "            numeric_columns = df.columns[1:]  # Exclude the 'Ürün' column\n",
    "            # Attempt to convert all numeric columns to float\n",
    "            for col in numeric_columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Drop rows where all numeric columns have NaN (i.e., non-convertible rows)\n",
    "            df_cleaned = df.dropna(subset=numeric_columns, how='all')\n",
    "            return df_cleaned\n",
    "\n",
    "        # Apply the conversion and cleaning process\n",
    "        df_cleaned = convert_to_float_and_drop_non_numeric(data.copy())\n",
    "\n",
    "        # Function to fill NaN values from both right to left and left to right\n",
    "        def fill_nan_both_directions(row):\n",
    "\n",
    "            filled_row = row[::-1].fillna(method='ffill')[::-1]\n",
    "\n",
    "            filled_row = filled_row.fillna(method='ffill')\n",
    "            return filled_row\n",
    "\n",
    "        def fill_nan_both_directions_corrected(df):\n",
    "            numeric_columns = df.columns[1:]  # Exclude the 'Ürün' column\n",
    "            df[numeric_columns] = df[numeric_columns].apply(fill_nan_both_directions, axis=1)\n",
    "            return df\n",
    "\n",
    "\n",
    "        df_filled_corrected = fill_nan_both_directions_corrected(df_cleaned)\n",
    "\n",
    "\n",
    "        ağırlıklar=pd.read_csv(\"ağırlıklar.csv\")\n",
    "        ağırlıklar=ağırlıklar.set_index(ağırlıklar[\"Ürün\"])\n",
    "        ağırlıklar=ağırlıklar.drop(\"Ürün\",axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        gfe=pd.read_csv(\"gfe.csv\")\n",
    "        gfe=gfe.set_index(pd.to_datetime(gfe[\"Tarih\"]))\n",
    "        gfe=gfe.drop(\"Tarih\",axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        data1=df_filled_corrected.copy()\n",
    "        degisim=(((data1.iloc[:,-1]/data1.iloc[:,1])-1)*100).fillna(0).groupby(level=0).mean().sort_index()\n",
    "\n",
    "\n",
    "\n",
    "        ağırlıklar[\"Değişim\"]=degisim\n",
    "\n",
    "\n",
    "\n",
    "        ağırlıklar[f\"Endeks_{bugün}\"]=ağırlıklar[\"Endeks_2024-10-11\"]*(1+(ağırlıklar[\"Değişim\"]/100))\n",
    "\n",
    "        ağırlıklar[f\"Ağırlıklı Endeks_{bugün}\"]=ağırlıklar[f\"Endeks_{bugün}\"]*ağırlıklar[\"Ağırlık\"]\n",
    "        gfe.loc[pd.to_datetime(bugün)]=ağırlıklar[f\"Ağırlıklı Endeks_{bugün}\"].sum()\n",
    "        gfe.to_csv(\"gfe.csv\",index=True)\n",
    "\n",
    "\n",
    "        endeks_sutunlari = ağırlıklar.filter(like='Endeks_')\n",
    "        endeksler = [col for col in ağırlıklar.columns if col.startswith('Endeks_')]\n",
    "        ağırlıklar[endeksler].to_csv(\"endeksler.csv\",index=True)\n",
    "\n",
    "\n",
    "        ağırlıklar.to_csv(\"ağırlıklar.csv\",index=True)\n",
    "\n",
    "\n",
    "        data1.to_csv(\"sepet.csv\")\n",
    "\n",
    "        tarih=datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "        tarih=pd.DataFrame({\"Current DateTime\": [tarih]})\n",
    "        tarih.to_csv(\"tarih.csv\")\n",
    "\n",
    "        import os\n",
    "        import subprocess\n",
    "        from datetime import datetime\n",
    "        import time\n",
    "        import git\n",
    "        from git import Repo\n",
    "        import os\n",
    "        repo_dir = \".git\"  # Buraya Git deposunun yolunu girin\n",
    "\n",
    "        def git_add_commit_push():\n",
    "            try:\n",
    "                # Repo nesnesini oluştur\n",
    "                repo = Repo(repo_dir)\n",
    "                assert not repo.bare\n",
    "\n",
    "                # Git add: tüm değişiklikleri ekliyoruz\n",
    "                repo.git.add(A=True)  # A=True ile tüm dosyalar eklenir\n",
    "\n",
    "                # Commit işlemi\n",
    "                commit_message = \"update\"\n",
    "                repo.index.commit(commit_message)\n",
    "                print(f\"Commit işlemi başarılı: {commit_message}\")\n",
    "\n",
    "                # Push işlemi\n",
    "                origin = repo.remote(name='origin')\n",
    "                origin.push()\n",
    "                print(\"Push işlemi başarılı.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Git işlemi sırasında hata oluştu: {e}\")\n",
    "\n",
    "            # Ana fonksiyonu çağırma\n",
    "        git_add_commit_push()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

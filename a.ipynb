{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime,timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "while True:\n",
    "\n",
    "    try:\n",
    "\n",
    "        bugün=datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "\n",
    "        dün=(datetime.now()-timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "        data=pd.read_csv(\"sepet.csv\")\n",
    "        data=data.set_index(data[\"Unnamed: 0\"]).drop(\"Unnamed: 0\",axis=1)\n",
    "        data.index.name=\"\"\n",
    "        try:\n",
    "            data=data.drop(f\"{bugün}\",axis=1)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "        def veriekle(urun, data, urunler_df):\n",
    "\n",
    "            try:\n",
    "\n",
    "                if urunler_df is None or urunler_df.empty:\n",
    "                    return data\n",
    "\n",
    "                elif isinstance(data.loc[urun], pd.Series):\n",
    "                    data_for_urun = data.loc[urun].to_frame().T  # Convert Series to DataFrame\n",
    "                else:\n",
    "                    data_for_urun = data.loc[urun]\n",
    "\n",
    "                # Merge the data with urunler_df\n",
    "                merged_df = pd.merge(data_for_urun, urunler_df, on='Ürün', how='outer')\n",
    "\n",
    "                # Index'i doğru ürün ismiyle dolduruyoruz\n",
    "                merged_df.index = len(merged_df) * [urun]\n",
    "\n",
    "                # Eğer _x ve _y ile aynı tarihli sütunlar varsa birleştiriyoruz\n",
    "                tarih_sutunlari = [col for col in merged_df.columns if col.endswith(\"_x\") or col.endswith(\"_y\")]\n",
    "                \n",
    "                for col in set([col.split(\"_\")[0] for col in tarih_sutunlari]):\n",
    "                    if col + \"_x\" in merged_df.columns and col + \"_y\" in merged_df.columns:\n",
    "                        # Sütunları birleştiriyoruz\n",
    "                        merged_df[col] = merged_df[col + \"_x\"].combine_first(merged_df[col + \"_y\"])\n",
    "                        # _x ve _y sütunlarını kaldırıyoruz\n",
    "                        merged_df = merged_df.drop(columns=[col + \"_x\", col + \"_y\"])\n",
    "\n",
    "                # Eski verileri (urun'e ait olan satırları) çıkarıyoruz\n",
    "                data_without_urun = data.drop(index=urun)\n",
    "\n",
    "                # Yeni verileri ekliyoruz\n",
    "                data = pd.concat([data_without_urun, merged_df])\n",
    "\n",
    "                # Data'yı index'e göre sıralıyoruz\n",
    "                data = data.sort_index()\n",
    "\n",
    "\n",
    "                return data\n",
    "            except:\n",
    "                return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # Function to scrape product names and prices from Migros for multiple URLs\n",
    "        def scrape_migros_products(pages):\n",
    "            product_data = []\n",
    "            \n",
    "            for page in pages:\n",
    "                print(f\"Scraping URL: {page}\")\n",
    "                driver.get(page)\n",
    "\n",
    "                try:\n",
    "                    # Wait for the product names to load\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '.product-name'))\n",
    "                    )\n",
    "\n",
    "                    # Get product names and prices\n",
    "                    products = driver.find_elements(By.CSS_SELECTOR, '.product-name')\n",
    "                    prices = driver.find_elements(By.CSS_SELECTOR, '.price-new')\n",
    "\n",
    "                    # Ensure we have matching names and prices\n",
    "                    if len(products) != len(prices):\n",
    "                        print(f\"Warning: Number of products and prices do not match on page {page}\")\n",
    "\n",
    "                    for i in range(len(products)):\n",
    "                        try:\n",
    "                            product_name = products[i].text.strip()\n",
    "                            product_price_text = prices[i].text.strip()\n",
    "                            product_price = clean_price(product_price_text)  # Convert to float\n",
    "                            product_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "                            print(f\"Product: {product_name}, Price: {product_price} TRY\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error scraping product {i} on page: {e}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading products on page: {e}\")\n",
    "\n",
    "                # Sleep to avoid being detected as a bot\n",
    "                sleep(2)\n",
    "            \n",
    "            return product_data\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/pirinc/c/1134?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\n",
    "            \"https://www.migros.com.tr/arama?q=pirin%C3%A7&kategori=5&sirala=indirim-yuzdesine-gore&sayfa=\",\"https://www.migros.com.tr/arama?q=pirin%C3%A7&kategori=5&sirala=indirim-yuzdesine-gore&sayfa=2\"\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Pirinç\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            data=veriekle(\"Pirinç\",data,urunler_df)\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # Function to scrape product names and prices from Migros for multiple URLs\n",
    "        def scrape_migros_products(pages):\n",
    "            product_data = []\n",
    "            \n",
    "            for page in pages:\n",
    "                print(f\"Scraping URL: {page}\")\n",
    "                driver.get(page)\n",
    "\n",
    "                try:\n",
    "                    # Wait for the product names to load\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '.product-name'))\n",
    "                    )\n",
    "\n",
    "                    # Get product names and prices\n",
    "                    products = driver.find_elements(By.CSS_SELECTOR, '.product-name')\n",
    "                    prices = driver.find_elements(By.CSS_SELECTOR, '.price-new')\n",
    "\n",
    "                    # Ensure we have matching names and prices\n",
    "                    if len(products) != len(prices):\n",
    "                        print(f\"Warning: Number of products and prices do not match on page {page}\")\n",
    "\n",
    "                    for i in range(len(products)):\n",
    "                        try:\n",
    "                            product_name = products[i].text.strip()\n",
    "                            product_price_text = prices[i].text.strip()\n",
    "                            product_price = clean_price(product_price_text)  # Convert to float\n",
    "                            product_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "                            print(f\"Product: {product_name}, Price: {product_price} TRY\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error scraping product {i} on page : {e}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading products on page : {e}\")\n",
    "\n",
    "                # Sleep to avoid being detected as a bot\n",
    "                sleep(2)\n",
    "            \n",
    "            return product_data\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/search?q=bu%C4%9Fday+unu%3AbestSeller%3AinStockFlag%3Atrue%3AproductPrimaryCategoryCode%3A1276\"]\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/arama?q=bu%C4%9Fday%20unu\"]\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        all_data = migros_data + carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if urunler_df is not None:\n",
    "            urunler_df = product_df.copy()\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Buğday Unu\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            data=veriekle(\"Buğday Unu\",data,urunler_df)\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # Function to scrape product names and prices from Migros\n",
    "        def scrape_migros_products(base_url, total_pages):\n",
    "            product_data = []\n",
    "            \n",
    "            for page in range(1, total_pages + 1):\n",
    "                url = f\"{base_url}{page}\"\n",
    "                print(f\"Scraping page {page}: {url}\")\n",
    "                driver.get(url)\n",
    "\n",
    "                try:\n",
    "                    # Wait for the product names to load\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '.product-name'))\n",
    "                    )\n",
    "\n",
    "                    # Get product names and prices\n",
    "                    products = driver.find_elements(By.CSS_SELECTOR, '.product-name')\n",
    "                    prices = driver.find_elements(By.CSS_SELECTOR, '.price-new')\n",
    "\n",
    "                    # Ensure we have matching names and prices\n",
    "                    if len(products) != len(prices):\n",
    "                        print(f\"Warning: Number of products and prices do not match on page {page}\")\n",
    "\n",
    "                    for i in range(len(products)):\n",
    "                        try:\n",
    "                            product_name = products[i].text.strip()\n",
    "                            product_price_text = prices[i].text.strip()\n",
    "                            product_price = clean_price(product_price_text)  # Convert to float\n",
    "                            product_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "                            print(f\"Product: {product_name}, Price: {product_price} TRY\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error scraping product {i} on page : {e}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading products on page : {e}\")\n",
    "\n",
    "                # Sleep to avoid being detected as a bot\n",
    "                sleep(2)\n",
    "            \n",
    "            return product_data\n",
    "\n",
    "\n",
    "        def scrape_carrefour_products():\n",
    "            carrefour_data=[]\n",
    "            pages=[\"https://www.carrefoursa.com/search?q=devam+s%C3%BCt%C3%BC%3AbestSeller%3AproductPrimaryCategoryCode%3A1848&show=All\"]\n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define base URL and total pages to scrape for Migros\n",
    "        migros_base_url = \"https://www.migros.com.tr/arama?q=bebek%20s%C3%BCt%C3%BC&kategori=70507&sirala=akilli-siralama&sayfa=\"\n",
    "        migros_total_pages = 2\n",
    "\n",
    "        migros_data = scrape_migros_products(migros_base_url, migros_total_pages)\n",
    "\n",
    "\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "        driver.quit()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df = product_df.copy()\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Bebek Sütü (Toz Karışım)\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Bebek Sütü (Toz Karışım)\",data,urunler_df)\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # Function to scrape product names and prices from Migros\n",
    "        def scrape_migros_products(base_url, total_pages):\n",
    "            product_data = []\n",
    "            \n",
    "            for page in range(1, total_pages + 1):\n",
    "                url = f\"{base_url}{page}\"\n",
    "                print(f\"Scraping page {page}: {url}\")\n",
    "                driver.get(url)\n",
    "\n",
    "                try:\n",
    "                    # Wait for the product names to load\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '.product-name'))\n",
    "                    )\n",
    "\n",
    "                    # Get product names and prices\n",
    "                    products = driver.find_elements(By.CSS_SELECTOR, '.product-name')\n",
    "                    prices = driver.find_elements(By.CSS_SELECTOR, '.price-new')\n",
    "\n",
    "                    # Ensure we have matching names and prices\n",
    "                    if len(products) != len(prices):\n",
    "                        print(f\"Warning: Number of products and prices do not match on page {page}\")\n",
    "\n",
    "                    for i in range(len(products)):\n",
    "                        try:\n",
    "                            product_name = products[i].text.strip()\n",
    "                            product_price_text = prices[i].text.strip()\n",
    "                            product_price = clean_price(product_price_text)  # Convert to float\n",
    "                            product_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "                            print(f\"Product: {product_name}, Price: {product_price} TRY\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error scraping product {i} on page : {e}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading products on page : {e}\")\n",
    "\n",
    "                # Sleep to avoid being detected as a bot\n",
    "                sleep(2)\n",
    "            \n",
    "            return product_data\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour\n",
    "        def scrape_carrefour_products():\n",
    "            carrefour_data=[]\n",
    "            pages = [\"https://www.carrefoursa.com/bulgur/c/1142?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define base URL and total pages to scrape for Migros\n",
    "        migros_base_url = \"https://www.migros.com.tr/arama?q=bulgur&kategori=1062&sirala=akilli-siralama&sayfa=\"\n",
    "        migros_total_pages = 2\n",
    "\n",
    "        # Scrape Migros products\n",
    "        migros_data = scrape_migros_products(migros_base_url, migros_total_pages)\n",
    "\n",
    "        # Scrape Carrefour products\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        all_data = migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "                urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "                urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "                urunler_df.index=len(urunler_df)*[\"Bulgur\"]\n",
    "                urunler_df=urunler_df.drop_duplicates()\n",
    "                urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "                data=veriekle(\"Bulgur\",data,urunler_df)\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # Function to scrape product names and prices from Migros\n",
    "        def scrape_migros_products(base_url, total_pages):\n",
    "            product_data = []\n",
    "            \n",
    "            for page in range(1, total_pages + 1):\n",
    "                url = f\"{base_url}{page}\"\n",
    "                print(f\"Scraping page {page}: {url}\")\n",
    "                driver.get(url)\n",
    "\n",
    "                try:\n",
    "                    # Wait for the product names to load\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '.product-name'))\n",
    "                    )\n",
    "\n",
    "                    # Get product names and prices\n",
    "                    products = driver.find_elements(By.CSS_SELECTOR, '.product-name')\n",
    "                    prices = driver.find_elements(By.CSS_SELECTOR, '.price-new')\n",
    "\n",
    "                    # Ensure we have matching names and prices\n",
    "                    if len(products) != len(prices):\n",
    "                        print(f\"Warning: Number of products and prices do not match on page {page}\")\n",
    "\n",
    "                    for i in range(len(products)):\n",
    "                        try:\n",
    "                            product_name = products[i].text.strip()\n",
    "                            product_price_text = prices[i].text.strip()\n",
    "                            product_price = clean_price(product_price_text)  # Convert to float\n",
    "                            product_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "                            print(f\"Product: {product_name}, Price: {product_price} TRY\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error scraping product {i} on page : {e}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading products on page : {e}\")\n",
    "\n",
    "                # Sleep to avoid being detected as a bot\n",
    "                sleep(2)\n",
    "            \n",
    "            return product_data\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour\n",
    "        def scrape_carrefour_products():\n",
    "            carrefour_data=[]\n",
    "            pages = [\"https://www.carrefoursa.com/search?q=ekmek%3AbestSeller%3AinStockFlag%3Atrue%3AproductPrimaryCategoryCode%3A1398\",\"https://www.carrefoursa.com/search?q=ekmek%3AbestSeller%3AinStockFlag%3Atrue%3AproductPrimaryCategoryCode%3A1401\"]\n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define base URL and total pages to scrape for Migros\n",
    "        migros_base_url = \"https://www.migros.com.tr/arama?q=ekmek&kategori=1109&markalar=492&sirala=akilli-siralama&sayfa=\"\n",
    "        migros_total_pages = 1\n",
    "\n",
    "        # Scrape Migros products\n",
    "        migros_data = scrape_migros_products(migros_base_url, migros_total_pages)\n",
    "\n",
    "        # Scrape Carrefour products\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Ekmek\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Ekmek\",data,urunler_df)\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        def scrape_migros_products(base_url, total_pages):\n",
    "            product_data = []\n",
    "            \n",
    "            for page in range(1, total_pages + 1):\n",
    "                url = f\"{base_url}{page}\"\n",
    "                print(f\"Scraping page {page}: {url}\")\n",
    "                driver.get(url)\n",
    "\n",
    "                try:\n",
    "                    # Wait for the product names to load\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '.product-name'))\n",
    "                    )\n",
    "\n",
    "                    # Get all product cards\n",
    "                    product_cards = driver.find_elements(By.CSS_SELECTOR, 'sm-list-page-item')\n",
    "\n",
    "                    for product_card in product_cards:\n",
    "                        # Check if the product is sponsored by looking for a specific sponsored class or attribute\n",
    "                        if \"external-list-item\" in product_card.get_attribute('class').lower():\n",
    "                            print(\"Skipping sponsored product.\")\n",
    "                            continue  # Skip this sponsored product\n",
    "\n",
    "                        try:\n",
    "                            # Extract the product name\n",
    "                            product_name = product_card.find_element(By.CSS_SELECTOR, '.product-name').text.strip()\n",
    "\n",
    "                            # Try to extract the price from multiple possible locations\n",
    "                            price_element = None\n",
    "                            try:\n",
    "                                # First attempt using the primary price selector\n",
    "                                price_element = product_card.find_element(By.CSS_SELECTOR, '.price-new')\n",
    "                            except Exception:\n",
    "                                try:\n",
    "                                    # If not found, try another potential selector for the price\n",
    "                                    price_element = product_card.find_element(By.CSS_SELECTOR, '.price .amount')\n",
    "                                except Exception:\n",
    "                                    print(f\"Price not found for product: {product_name}\")\n",
    "                                    continue  # Skip if no price is found\n",
    "\n",
    "                            if price_element:\n",
    "                                product_price_text = price_element.text.strip()\n",
    "                                product_price = clean_price(product_price_text)  # Convert price to float\n",
    "                                # Append product name and price to the list\n",
    "                                product_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "                                print(f\"Product: {product_name}, Price: {product_price} TRY\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error scraping a product on page: {e}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading products on page : {e}\")\n",
    "\n",
    "                # Sleep to avoid being detected as a bot\n",
    "                sleep(2)\n",
    "            \n",
    "            return product_data\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour\n",
    "        def scrape_carrefour_products():\n",
    "                carrefour_data=[]\n",
    "                url = \"https://www.carrefoursa.com/search?q=bisk%C3%BCvi%3AbestSeller%3AproductPrimaryCategoryCode%3A1529%3AinStockFlag%3Atrue&show=All\"\n",
    "\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "                return carrefour_data\n",
    "\n",
    "        # Define base URL and total pages to scrape for Migros\n",
    "        migros_base_url = \"https://www.migros.com.tr/arama?q=bisk%C3%BCvi&kategori=1084&sayfa=\"\n",
    "        migros_total_pages = 8\n",
    "\n",
    "        # Scrape Migros products\n",
    "        migros_data = scrape_migros_products(migros_base_url, migros_total_pages)\n",
    "\n",
    "        # Scrape Carrefour products\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "        driver.quit()\n",
    "        \n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df[urunler_df['Ürün'].str.contains(\"Bisküvi\", case=False)]\n",
    "\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Bisküvi\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Bisküvi\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # Function to scrape product names and prices from Migros for multiple URLs\n",
    "        def scrape_migros_products(pages):\n",
    "            product_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "                driver.get(url)\n",
    "\n",
    "                try:\n",
    "                    # Wait for the product names to load\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '.product-name'))\n",
    "                    )\n",
    "\n",
    "                    # Get all product cards\n",
    "                    product_cards = driver.find_elements(By.CSS_SELECTOR, 'sm-list-page-item')\n",
    "\n",
    "                    for product_card in product_cards:\n",
    "                        # Check if the product is sponsored by looking for a specific sponsored class or attribute\n",
    "                        if \"external-list-item\" in product_card.get_attribute('class').lower():\n",
    "                            print(\"Skipping sponsored product.\")\n",
    "                            continue  # Skip this sponsored product\n",
    "\n",
    "                        try:\n",
    "                            # Extract the product name\n",
    "                            product_name = product_card.find_element(By.CSS_SELECTOR, '.product-name').text.strip()\n",
    "\n",
    "                            # Try to extract the price from multiple possible locations\n",
    "                            price_element = None\n",
    "                            try:\n",
    "                                # First attempt using the primary price selector\n",
    "                                price_element = product_card.find_element(By.CSS_SELECTOR, '.price-new')\n",
    "                            except Exception:\n",
    "                                try:\n",
    "                                    # If not found, try another potential selector for the price\n",
    "                                    price_element = product_card.find_element(By.CSS_SELECTOR, '.price .amount')\n",
    "                                except Exception:\n",
    "                                    print(f\"Price not found for product: {product_name}\")\n",
    "                                    continue  # Skip if no price is found\n",
    "\n",
    "                            if price_element:\n",
    "                                product_price_text = price_element.text.strip()\n",
    "                                product_price = clean_price(product_price_text)  # Convert price to float\n",
    "                                # Append product name and price to the list\n",
    "                                product_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "                                print(f\"Product: {product_name}, Price: {product_price} TRY\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error scraping a product on page: {e}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading products on page: {e}\")\n",
    "\n",
    "                # Sleep to avoid being detected as a bot\n",
    "                sleep(2)\n",
    "            \n",
    "            return product_data\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/search?q=kraker%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/arama?q=kraker&sayfa=1&kategori=10218&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=kraker&sayfa=2&kategori=10218&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=kraker&sayfa=3&kategori=10218&sirala=akilli-siralama\"]\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Kraker\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Kraker\",data,urunler_df)\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # Function to scrape product names and prices from Migros for multiple URLs\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/search?q=gofret%3AbestSeller%3AinStockFlag%3Atrue&text=gofret#\"]\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/arama?q=gofret&sayfa=1&kategori=1082&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=gofret&sayfa=2&kategori=1082&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=gofret&sayfa=3&kategori=1082&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=gofret&sayfa=4&kategori=1082&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=gofret&sayfa=5&kategori=1082&sirala=akilli-siralama\"]\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Gofret\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            data=veriekle(\"Gofret\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/pastalar/c/1289?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/arama?q=pasta&sayfa=1&kategori=1113\",\n",
    "                        \"https://www.migros.com.tr/arama?q=pasta&sayfa=1&kategori=1111\"]\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Pasta\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Pasta\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/search?q=kek%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/arama?q=kek&sayfa=1&kategori=1085\"]\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Kek\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Kek\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/search?q=baklava%3AbestSeller%3AinStockFlag%3Atrue%3AproductPrimaryCategoryCode%3A1294\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/arama?q=baklava&sayfa=1&kategori=126\"\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Baklava\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Baklava\",data,urunler_df)\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/baliklar/c/1099?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/arama?q=yufka\"\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        all_data = migros_data \n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Ekmek Hamuru (Yufka)\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Ekmek Hamuru (Yufka)\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/makarna/c/1122?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/arama?q=makarna&sayfa=1&kategori=10112&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=makarna&sayfa=2&kategori=10112&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=makarna&sayfa=3&kategori=10112&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=makarna&sayfa=4&kategori=10112&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=makarna&sayfa=5&kategori=10112&sirala=akilli-siralama\"\n",
    "                        \n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Şehriye\", regex=True)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Makarna\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Makarna\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # Function to scrape product names and prices from Migros for multiple URLs\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/search?q=%C5%9Fehriye%3AbestSeller%3AinStockFlag%3Atrue%3AproductPrimaryCategoryCode%3A1122\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/arama?q=%C5%9Fehriye&sayfa=1&kategori=5\"\n",
    "        ]\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Makarna\", regex=True)]\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Bulgur\", regex=True)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Şehriye\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            data=veriekle(\"Şehriye\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        # Function to scrape product names and prices from Migros for multiple URLs\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/musli-hububat-urunleri/c/1378?q=%3AbestSeller%3Acategory%3A1310%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/kahvaltilik-gevrek-c-422?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/kahvaltilik-gevrek-c-422?sayfa=2&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/kahvaltilik-gevrek-c-422?sayfa=3&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/kahvaltilik-gevrek-c-422?sayfa=4&sirala=onerilenler\"\n",
    "                        \n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Tahıl Gevreği\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Tahıl Gevreği\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/search?q=dana+eti%3AbestSeller%3AinStockFlag%3Atrue%3AproductPrimaryCategoryCode%3A1046\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/dana-eti-c-3fa?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/dana-eti-c-3fa?sayfa=2&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/dana-eti-c-3fa?sayfa=3&sirala=onerilenler\",\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Dana Eti\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Dana Eti\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/kuzu/c/1054?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/kuzu-eti-c-3fb?sayfa=1\"\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Kuzu Eti\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Kuzu Eti\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/pilic/c/1061?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/pilic-c-3fe?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/pilic-c-3fe?sayfa=2&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/pilic-c-3fe?sayfa=3&sirala=onerilenler\",\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Tavuk Eti\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Tavuk Eti\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/sakatat-c-3fd\"\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        all_data = migros_data \n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Sakatat\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Sakatat\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            pages = [\"https://www.carrefoursa.com/sucuk/c/1077?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/sucuk-c-404?sayfa=2&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/sucuk-c-404?sayfa=1&sirala=onerilenler\",\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Sucuk\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Sucuk\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/sosis/c/1084?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/sosis-c-405?sayfa=1\"\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Salam\", regex=True)]\n",
    "            urunler_df.index=len(urunler_df)*[\"Sosis\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Sosis\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/salam-jambon-ve-fume/c/1092?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/salam-c-112d6?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/salam-c-112d6?sayfa=2&sirala=onerilenler\",\n",
    "                \n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Salam\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Salam\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/pratik-yemek-c-44f?sayfa=1&90=503&sirala=onerilenler\"\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        all_data = migros_data \n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Hazır Et Yemekleri\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Hazır Et Yemekleri\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/baliklar/c/1099?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/mevsim-baliklari-c-402?sayfa=1\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Balık\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Balık\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/paketli-urunler/c/1068?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/dondurulmus-deniz-urunleri-c-2830?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/dondurulmus-deniz-urunleri-c-2830?sayfa=2&sirala=onerilenler\"\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Konserve Balık\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Konserve Balık\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/sut/c/1311?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/sut-c-6c?sayfa=1&109=1020&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/sut-c-6c?sayfa=2&109=1020&sirala=onerilenler\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "            urunler_df.index=len(urunler_df)*[\"Süt\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Süt\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/yogurt/c/1389?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/yogurt-c-6e?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/yogurt-c-6e?sayfa=2&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/yogurt-c-6e?sayfa=3&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/yogurt-c-6e?sayfa=4&sirala=onerilenler\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Yoğurt\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Yoğurt\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/sutlu-tatli-puding/c/1962?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/geleneksel-sutlu-tatlilar-c-2765?sayfa=1&sirala=onerilenler\"\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Hazır Sütlü Tatlılar\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            data=veriekle(\"Hazır Sütlü Tatlılar\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/beyaz-peynir/c/1319?q=%3AbestSeller&show=All\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/inek-peyniri-c-2731?sayfa=1\",\n",
    "                        \"https://www.migros.com.tr/koyun-peyniri-c-2732?sayfa=1\",\n",
    "                        \"https://www.migros.com.tr/suzme-peynir-c-2733?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/suzme-peynir-c-2733?sayfa=2&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/keci-peyniri-c-2735?sayfa=1\"\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Beyaz Peynir\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Beyaz Peynir\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/kasar-/c/1324?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/kasar-peyniri-c-40d?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/kasar-peyniri-c-40d?sayfa=2&sirala=onerilenler\"\n",
    "                \n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Kaşar Peyniri\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Kaşar Peyniri\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/krem-peynir/c/1336?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/arama?q=krem%20peynir&sayfa=1&kategori=10039&sirala=akilli-siralama\",\n",
    "                        \"https://www.migros.com.tr/arama?q=krem%20peynir&sayfa=2&kategori=10039&sirala=akilli-siralama\"\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Krem Peynir\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            data=veriekle(\"Krem Peynir\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/yumurta/c/1349?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =   [\"https://www.migros.com.tr/yumurta-c-70\"\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Yumurta\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            data=veriekle(\"Yumurta\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/tereyag/c/1350?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/tereyagi-c-413?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/tereyagi-c-413?sayfa=2&sirala=onerilenler\"\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Tereyağı (Kahvaltılık)\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Tereyağı (Kahvaltılık)\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/margarin/c/1351?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/margarin-c-414?sayfa=1\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Margarin\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            data=veriekle(\"Margarin\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/zeytinyagi/c/1114?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  =  [\"https://www.migros.com.tr/zeytinyagi-c-433?sayfa=1&sirala=onerilenler\",\n",
    "                        \"https://www.migros.com.tr/zeytinyagi-c-433?sayfa=2&sirala=onerilenler\"\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Zeytinyağı\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Zeytinyağı\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/aycicek/c/1112?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/aycicek-yagi-c-42d?sayfa=1\"\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "        # Combine both datasets into one DataFrame\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Ayçiçek Yağı\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            data=veriekle(\"Ayçiçek Yağı\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        from time import sleep\n",
    "        import pandas as pd\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "        options = Options()\n",
    "        options.headless = False  # Tarayıcı görünür modda çalışacak\n",
    "\n",
    "        # WebDriver'ı başlat\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Function to clean and convert price text to float\n",
    "        def clean_price(price_text):\n",
    "            # Remove any non-numeric characters except for commas and dots\n",
    "            price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "            # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "            price_text = price_text.replace(',', '.')\n",
    "            try:\n",
    "                return float(price_text)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "        def scrape_carrefour_products():\n",
    "            \n",
    "            pages = [\"https://www.carrefoursa.com/search?q=portakal%3AbestSeller%3AproductPrimaryCategoryCode%3A1016%3AinStockFlag%3Atrue&text=portakal#\"]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            carrefour_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "            \n",
    "                    # Send a GET request to fetch the HTML content\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the HTML content\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Find all product cards\n",
    "                    product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                    \n",
    "                    # Loop through each product card and extract the product name and price\n",
    "                    for index, product_card in enumerate(product_cards):\n",
    "                        # Extract product name\n",
    "                        product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                        \n",
    "                        # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                        product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                        \n",
    "                        # Convert price to float\n",
    "                        try:\n",
    "                            product_price = float(product_price)\n",
    "                        except ValueError:\n",
    "                            product_price = None  # Handle cases where the price might not be convertible\n",
    "                        \n",
    "                        # Print product name and price\n",
    "                        print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                        carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "            \n",
    "            return carrefour_data\n",
    "\n",
    "        # Define the list of pages to scrape from Migros\n",
    "        migros_pages  = [\"https://www.migros.com.tr/narenciye-c-3ec?sayfa=1&93=652&sirala=onerilenler\"\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Scrape products from Migros and Carrefour\n",
    "        migros_data = scrape_migros_products(migros_pages)\n",
    "        carrefour_data = scrape_carrefour_products()\n",
    "\n",
    "\n",
    "        if not migros_data:\n",
    "            all_data =   carrefour_data\n",
    "        elif not carrefour_data:\n",
    "            all_data=migros_data\n",
    "        else:\n",
    "            all_data=migros_data+carrefour_data\n",
    "        product_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        urunler_df = product_df.copy()\n",
    "        if urunler_df is not None:\n",
    "            urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "            urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "            urunler_df.index=len(urunler_df)*[\"Portakal\"]\n",
    "            urunler_df=urunler_df.drop_duplicates()\n",
    "            urunler_df=urunler_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #data=pd.concat([data,urunler_df],axis=0)\n",
    "\n",
    "\n",
    "            data=veriekle(\"Portakal\",data,urunler_df)\n",
    "\n",
    "\n",
    "        def vericek(carrefour=\"\",migros=\"\",name=\"\"):\n",
    "            try:  \n",
    "\n",
    "            # Function to clean and convert price text to float\n",
    "                def clean_price(price_text):\n",
    "                    # Remove any non-numeric characters except for commas and dots\n",
    "                    price_text = re.sub(r'[^\\d,.]', '', price_text)\n",
    "                    # Replace commas with dots if needed (ensure it works with Turkish formatted numbers)\n",
    "                    price_text = price_text.replace(',', '.')\n",
    "                    try:\n",
    "                        return float(price_text)\n",
    "                    except ValueError:\n",
    "                        return None\n",
    "\n",
    "\n",
    "\n",
    "                # Function to scrape product names and prices from Carrefour for multiple URLs\n",
    "                def scrape_carrefour_products(carrefour):\n",
    "                    \n",
    "                    pages = carrefour\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "                    carrefour_data = []\n",
    "                    \n",
    "                    for url in pages:\n",
    "                    \n",
    "                            # Send a GET request to fetch the HTML content\n",
    "                        response = requests.get(url)\n",
    "\n",
    "                        # Check if the request was successful\n",
    "                        if response.status_code == 200:\n",
    "                            # Parse the HTML content\n",
    "                            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                            \n",
    "                            # Find all product cards\n",
    "                            product_cards = soup.find_all(\"div\", class_=\"productCardData\")\n",
    "                            \n",
    "                            # Loop through each product card and extract the product name and price\n",
    "                            for index, product_card in enumerate(product_cards):\n",
    "                                # Extract product name\n",
    "                                product_name = product_card.find(\"input\", {\"id\": \"productNamePost\"})['value']\n",
    "                                \n",
    "                                # Extract product price (from the input tag with id=\"productPricePost\")\n",
    "                                product_price = product_card.find(\"input\", {\"id\": \"productPricePost\"})['value']\n",
    "                                \n",
    "                                # Convert price to float\n",
    "                                try:\n",
    "                                    product_price = float(product_price)\n",
    "                                except ValueError:\n",
    "                                    product_price = None  # Handle cases where the price might not be convertible\n",
    "                                \n",
    "                                # Print product name and price\n",
    "                                print(f\"Product {index + 1}: {product_name}, Price: {product_price} TL\")\n",
    "                                carrefour_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "\n",
    "                        else:\n",
    "                            print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "                    \n",
    "                    return carrefour_data\n",
    "\n",
    "                # Define the list of pages to scrape from Migros\n",
    "                migros_pages  = migros\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                if migros:\n",
    "                    migros_data = scrape_migros_products(migros)\n",
    "                if carrefour:\n",
    "                    carrefour_data = scrape_carrefour_products(carrefour)\n",
    "\n",
    "                if migros and carrefour:\n",
    "\n",
    "                    all_data = migros_data + carrefour_data\n",
    "                elif migros and not carrefour:\n",
    "                    all_data=migros_data\n",
    "                else:\n",
    "                    all_data=carrefour_data\n",
    "                product_df = pd.DataFrame(all_data)\n",
    "\n",
    "                # Close the browser\n",
    "                driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                urunler_df = product_df.copy()\n",
    "                if urunler_df is not None:\n",
    "                    urunler_df.columns=[\"Ürün\",str(bugün)]\n",
    "                    urunler_df=urunler_df.groupby(\"Ürün\", as_index=False).agg({str(bugün): 'mean'})\n",
    "\n",
    "                    urunler_df.index=len(urunler_df)*[name]\n",
    "                    urunler_df=urunler_df.drop_duplicates()\n",
    "                    urunler_df=urunler_df.dropna()\n",
    "\n",
    "                    return urunler_df\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "                \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=%C3%BCz%C3%BCm%3AbestSeller%3AproductPrimaryCategoryCode%3A1017%3AinStockFlag%3Atrue&text=%C3%BCz%C3%BCm#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=%C3%BCz%C3%BCm&sayfa=1&kategori=101\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Üzüm\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Üzüm\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=armut%3AbestSeller%3AproductPrimaryCategoryCode%3A1018%3AinStockFlag%3Atrue&text=armut#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=armut&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Armut\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Armut\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=ayva%3AbestSeller%3AproductPrimaryCategoryCode%3A1018%3AinStockFlag%3Atrue&text=ayva#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=ayva&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Ayva\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Ayva\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=%C3%A7ilek%3AbestSeller%3AproductPrimaryCategoryCode%3A1017%3AinStockFlag%3Atrue&text=%C3%A7ilek#\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=carrefour,name=\"Çilek\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Çilek\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=elma%3AbestSeller%3AproductPrimaryCategoryCode%3A1018%3AinStockFlag%3Atrue&text=elma#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=elma&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Elma\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Elma\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=karpuz%3AbestSeller%3AinStockFlag%3Atrue&text=karpuz#\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=carrefour,name=\"Karpuz\")\n",
    "        urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Karpuz kg|Karpuz Kg\")]\n",
    "\n",
    "\n",
    "        data=veriekle(\"Karpuz\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=kavun%3AbestSeller%3AinStockFlag%3Atrue%3AproductPrimaryCategoryCode%3A1018\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=kavun&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kavun\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Kavun\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=kivi%3AbestSeller%3AinStockFlag%3Atrue&text=kivi#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=kivi&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kivi\")\n",
    "        urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"ml\")]\n",
    "\n",
    "\n",
    "        data=veriekle(\"Kivi\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=limon%3AbestSeller%3AproductPrimaryCategoryCode%3A1016%3AinStockFlag%3Atrue&text=limon#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=limon&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Limon\")\n",
    "\n",
    "        data=veriekle(\"Limon\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=mandalina%3AbestSeller%3AinStockFlag%3Atrue&text=mandalina#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=mandalina&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Mandalina\")\n",
    "        urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"kg|Kg\")]\n",
    "\n",
    "\n",
    "\n",
    "        data=veriekle(\"Mandalina\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=muz%3AbestSeller%3AproductPrimaryCategoryCode%3A1022%3AinStockFlag%3Atrue&text=muz#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=muz&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Muz\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Muz\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=%C5%9Feftali%3AbestSeller%3AproductPrimaryCategoryCode%3A1018%3AinStockFlag%3Atrue&text=%C5%9Feftali#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=%C5%9Feftali&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Şeftali\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Şeftali\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=nar%3AbestSeller%3AproductPrimaryCategoryCode%3A1018%3AinStockFlag%3Atrue&text=nar#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=nar&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Nar\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Nar\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=badem%3AbestSeller%3AproductPrimaryCategoryCode%3A1519%3AinStockFlag%3Atrue&text=badem#\"]\n",
    "        migros=[\"https://www.migros.com.tr/badem-c-280f\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Badem İçi\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Badem İçi\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=ceviz%3AbestSeller%3AproductPrimaryCategoryCode%3A1519%3AinStockFlag%3Atrue&text=ceviz#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=ceviz&sayfa=1&kategori=1089\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Ceviz İçi\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Ceviz İçi\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=f%C4%B1nd%C4%B1k%3AbestSeller%3AproductPrimaryCategoryCode%3A1519%3AinStockFlag%3Atrue&text=f%C4%B1nd%C4%B1k#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=f%C4%B1nd%C4%B1k&sayfa=1&kategori=1090\",\"https://www.migros.com.tr/arama?q=f%C4%B1nd%C4%B1k&sayfa=1&kategori=1089\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Fındık İçi\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Fındık İçi\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=Antep%20F%C4%B1st%C4%B1%C4%9F%C4%B1&sayfa=1&kategori=1090\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=\"\",migros=migros,name=\"Antep Fıstığı\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Antep Fıstığı\",data,urunler_df)\n",
    "\n",
    "\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=Yer%20F%C4%B1st%C4%B1%C4%9F%C4%B1&sayfa=1&kategori=1090\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=\"\",migros=migros,name=\"Yer Fıstığı\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Yer Fıstığı\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=leblebi%3AbestSeller%3AinStockFlag%3Atrue&text=leblebi#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=leblebi&sayfa=1&kategori=70651\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Leblebi\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Leblebi\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=ay%C3%A7ekirde%C4%9Fi%3AbestSeller%3AinStockFlag%3Atrue&text=ay%C3%A7ekirde%C4%9Fi#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=ay%20%C3%A7ekirde%C4%9Fi\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Ay Çekirdeği\")\n",
    "        urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Ayçekirdeği|Ayçekirdek|Şimşek\")]\n",
    "\n",
    "\n",
    "\n",
    "        data=veriekle(\"Ay Çekirdeği\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=kabak+%C3%A7ekirde%C4%9Fi%3AbestSeller%3AinStockFlag%3Atrue&text=kabak+%C3%A7ekirde%C4%9Fi#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=kabak%20%C3%A7ekirde%C4%9Fi&sayfa=1&kategori=70651\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kabak Çekirdeği\")\n",
    "        data.loc[\"Kabak Çekirdeği\"]=urunler_df\n",
    "\n",
    "        #data=veriekle(\"Kabak Çekirdeği\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=Kuru+%C3%9Cz%C3%BCm%3AbestSeller%3AinStockFlag%3Atrue%3AproductPrimaryCategoryCode%3A1519\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=Kuru%20%C3%9Cz%C3%BCm\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kuru Üzüm\")\n",
    "        urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"üzüm|Üzüm\")]\n",
    "\n",
    "        data=veriekle(\"Kuru Üzüm\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=kuru+kay%C4%B1s%C4%B1%3AbestSeller%3AproductPrimaryCategoryCode%3A1519%3AinStockFlag%3Atrue&text=kuru+kay%C4%B1s%C4%B1#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=kuru%20kay%C4%B1s%C4%B1\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kuru Kayısı\")\n",
    "        urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"kayısı|Kayısı\")]\n",
    "        \n",
    "\n",
    "\n",
    "        data=veriekle(\"Kuru Kayısı\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=%C3%A7arliston%3AbestSeller%3AproductPrimaryCategoryCode%3A1027%3AinStockFlag%3Atrue&text=%C3%A7arliston#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=%C3%A7arliston&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Çarliston Biber\")\n",
    "        urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Çarliston|çarliston\")]\n",
    "\n",
    "        data=veriekle(\"Çarliston Biber\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=Dolmal%C4%B1k+Biber%3AbestSeller%3AproductPrimaryCategoryCode%3A1027\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=Dolmal%C4%B1k%20Biber&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Dolmalık Biber\")\n",
    "        urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Dolma|Dolmalık\")]\n",
    "\n",
    "\n",
    "        data=veriekle(\"Dolmalık Biber\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=Sivri+Biber%3AbestSeller%3AproductPrimaryCategoryCode%3A1027%3AinStockFlag%3Atrue&text=Sivri+Biber#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=Sivri%20Biber\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Sivri Biber\")\n",
    "        urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"sivri|Sivri\")]\n",
    "\n",
    "        data=veriekle(\"Sivri Biber\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=dereotu%3AbestSeller%3AinStockFlag%3Atrue&text=dereotu#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=dereotu\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Dereotu\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Dereotu\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=domates%3AbestSeller%3AinStockFlag%3Atrue&text=domates#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=domates&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Domates\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Salçası\", regex=True)]\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"kg\", regex=True)]\n",
    "\n",
    "            data=veriekle(\"Domates\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=taze+fasulye%3AbestSeller%3AinStockFlag%3Atrue%3AproductPrimaryCategoryCode%3A1031\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=taze%20fasulye&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Taze Fasulye\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Taze Fasulye\",data,urunler_df)\n",
    "\n",
    "\n",
    "        def scrape_migros_products(pages):\n",
    "            product_data = []\n",
    "            \n",
    "            for url in pages:\n",
    "                driver.get(url)\n",
    "\n",
    "                try:\n",
    "                    # Wait for the product names to load\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '.product-name'))\n",
    "                    )\n",
    "\n",
    "                    # Get all product cards\n",
    "                    product_cards = driver.find_elements(By.CSS_SELECTOR, 'sm-list-page-item')\n",
    "\n",
    "                    for product_card in product_cards:\n",
    "                        # Check if the product is sponsored by looking for a specific sponsored class or attribute\n",
    "                        if \"external-list-item\" in product_card.get_attribute('class').lower():\n",
    "                            print(\"Skipping sponsored product.\")\n",
    "                            continue  # Skip this sponsored product\n",
    "\n",
    "                        try:\n",
    "                            # Extract the product name\n",
    "                            product_name = product_card.find_element(By.CSS_SELECTOR, '.product-name').text.strip()\n",
    "\n",
    "                            # Try to extract the price from multiple possible locations\n",
    "                            price_element = None\n",
    "                            try:\n",
    "                                # First attempt using the primary price selector\n",
    "                                price_element = product_card.find_element(By.CSS_SELECTOR, '.price-new')\n",
    "                            except Exception:\n",
    "                                try:\n",
    "                                    # If not found, try another potential selector for the price\n",
    "                                    price_element = product_card.find_element(By.CSS_SELECTOR, '.price .amount')\n",
    "                                except Exception:\n",
    "                                    print(f\"Price not found for product: {product_name}\")\n",
    "                                    continue  # Skip if no price is found\n",
    "\n",
    "                            if price_element:\n",
    "                                product_price_text = price_element.text.strip()\n",
    "                                product_price = clean_price(product_price_text)  # Convert price to float\n",
    "                                # Append product name and price to the list\n",
    "                                product_data.append({\"Product Name\": product_name, \"Price (TRY)\": product_price})\n",
    "                                print(f\"Product: {product_name}, Price: {product_price} TRY\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error scraping a product on page: {e}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading products on page: {e}\")\n",
    "\n",
    "                # Sleep to avoid being detected as a bot\n",
    "                sleep(2)\n",
    "            \n",
    "            return product_data\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=havu%C3%A7%3AbestSeller%3Acategory%3A1014\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=havu%C3%A7&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Havuç\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Havuç\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=%C4%B1spanak%3AbestSeller%3AproductPrimaryCategoryCode%3A1030%3AinStockFlag%3Atrue&text=%C4%B1spanak#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=%C4%B1spanak&sayfa=1&kategori=2&markalar=492&sirala=akilli-siralama\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Ispanak\")\n",
    "        urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Ispanak\")]\n",
    "\n",
    "        data=veriekle(\"Ispanak\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=kabak%3AbestSeller%3AinStockFlag%3Atrue&text=kabak#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=kabak\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kabak\")\n",
    "        urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Sakız|Dolmalık|Adet\")]\n",
    "\n",
    "        data=veriekle(\"Kabak\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=karnabahar%3AbestSeller%3Acategory%3A1014%3AinStockFlag%3Atrue&text=karnabahar#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=karnabahar&sayfa=1&markalar=492&sirala=akilli-siralama\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Karnabahar\")\n",
    "        urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Karnabahar\")]\n",
    "\n",
    "\n",
    "        data=veriekle(\"Karnabahar\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=kuru+so%C4%9Fan%3AbestSeller%3AproductPrimaryCategoryCode%3A1033%3AinStockFlag%3Atrue&text=kuru+so%C4%9Fan#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=kuru%20so%C4%9Fan\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kuru Soğan\")\n",
    "        urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"soğan|Soğan\")]\n",
    "\n",
    "        data=veriekle(\"Kuru Soğan\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=beyaz%20lahana&sayfa=1&markalar=492&sirala=akilli-siralama\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=\"\",migros=migros,name=\"Beyaz Lahana\")\n",
    "\n",
    "\n",
    "\n",
    "        data=veriekle(\"Beyaz Lahana\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=k%C4%B1rm%C4%B1z%C4%B1%20lahana&sayfa=1&markalar=492&sirala=akilli-siralama\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=\"\",migros=migros,name=\"Kırmızı Lahana\")\n",
    "\n",
    "\n",
    "        data=veriekle(\"Kırmızı Lahana\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=mantar%3AbestSeller%3AinStockFlag%3Atrue&text=mantar#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=mantar&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Mantar\")\n",
    "        urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Mantar|mantar|mantarı|Mantarı\")]\n",
    "        urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Çorba|Pano|İstiridye|Salatası\")]\n",
    "\n",
    "        data=veriekle(\"Mantar\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=k%C4%B1v%C4%B1rc%C4%B1k%3AbestSeller%3AinStockFlag%3Atrue&text=k%C4%B1v%C4%B1rc%C4%B1k#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=k%C4%B1v%C4%B1rc%C4%B1k&sayfa=1&markalar=492&sirala=akilli-siralama\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kıvırcık\")\n",
    "        urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"kıvırcık|Kıvırcık\")]\n",
    "        data=veriekle(\"Kıvırcık\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=maydonoz%3AbestSeller%3AinStockFlag%3Atrue&text=maydonoz#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=maydonoz&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Maydanoz\")\n",
    "\n",
    "        data=veriekle(\"Maydanoz\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=nane%3AbestSeller%3AproductPrimaryCategoryCode%3A1030%3AinStockFlag%3Atrue&text=nane#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=nane&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Nane\")\n",
    "\n",
    "        data=veriekle(\"Nane\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=patl%C4%B1can%3AbestSeller%3AproductPrimaryCategoryCode%3A1031%3AinStockFlag%3Atrue&text=patl%C4%B1can#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=patl%C4%B1can&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Patlıcan\")\n",
    "\n",
    "        data=veriekle(\"Patlıcan\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=p%C4%B1rasa%3AbestSeller%3AinStockFlag%3Atrue&text=p%C4%B1rasa#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=p%C4%B1rasa\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Pırasa\")\n",
    "        urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"pırasa|Pırasa\")]\n",
    "\n",
    "        data=veriekle(\"Pırasa\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=roka%3AbestSeller%3AinStockFlag%3Atrue&text=roka#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=roka&sayfa=1&markalar=492&sirala=akilli-siralama\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Roka\")\n",
    "        urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"roka|Roka\")]\n",
    "        data=veriekle(\"Roka\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=salatal%C4%B1k%3AbestSeller%3Acategory%3A1014%3AinStockFlag%3Atrue&text=salatal%C4%B1k#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=salatal%C4%B1k&sayfa=1&markalar=492&sirala=akilli-siralama&kategori=102\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Salatalık\")\n",
    "\n",
    "        data=veriekle(\"Salatalık\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/kuru-sarimsak-kg-p-30024962\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=sar%C4%B1msak&sayfa=1&kategori=2\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Sarımsak\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Sarımsak\", regex=True)]\n",
    "\n",
    "            data=veriekle(\"Sarımsak\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=k%C4%B1rm%C4%B1z%C4%B1%20turp\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=\"\",migros=migros,name=\"Kırmızı Turp\")\n",
    "        data=veriekle(\"Kırmızı Turp\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=patates%3AbestSeller%3AproductPrimaryCategoryCode%3A1033%3AinStockFlag%3Atrue&text=patates#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=patates&sayfa=1&kategori=1014&markalar=492&sirala=akilli-siralama\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Patates\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Sarımsak|Soğan\", regex=True)]\n",
    "            data=veriekle(\"Patates\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=kuru+fasulye%3AbestSeller%3AinStockFlag%3Atrue%3Acategory%3A1110\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=kuru%20fasulye&sayfa=1&kategori=5\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kuru Fasulye\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"fasulye|Fasulye\")]\n",
    "\n",
    "            data=veriekle(\"Kuru Fasulye\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=nohut%3AbestSeller%3AproductPrimaryCategoryCode%3A1152%3AinStockFlag%3Atrue&text=nohut#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=nohut&sayfa=1&kategori=10136\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Nohut\")\n",
    "        data=veriekle(\"Nohut\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=mercimek%3AbestSeller%3AproductPrimaryCategoryCode%3A1152%3AinStockFlag%3Atrue&text=mercimek#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=mercimek&sayfa=1&kategori=70601\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Mercimek\")\n",
    "        data=veriekle(\"Mercimek\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/diger-sebze/c/1193?q=%3AbestSeller&show=All\",\"https://www.carrefoursa.com/yesil-sebze/c/1187?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=konserve&sayfa=1&kategori=10324\",\n",
    "                \"https://www.migros.com.tr/arama?q=konserve&sayfa=1&kategori=10304\",\n",
    "                \"https://www.migros.com.tr/arama?q=konserve&sayfa=1&kategori=10325\",\n",
    "                \"https://www.migros.com.tr/arama?q=konserve&sayfa=1&kategori=10328\",\n",
    "                \"https://www.migros.com.tr/arama?q=konserve&sayfa=1&kategori=10326\",\n",
    "                \"https://www.migros.com.tr/arama?q=konserve&sayfa=1&kategori=10322\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Konserveler\")\n",
    "        data=veriekle(\"Konserveler\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=tur%C5%9Fu%3AbestSeller%3AinStockFlag%3Atrue&text=tur%C5%9Fu#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=tur%C5%9Fu&sayfa=1&kategori=1108&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=tur%C5%9Fu&sayfa=2&kategori=1108&sirala=akilli-siralama\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Turşu\")\n",
    "        data=veriekle(\"Turşu\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/salca/c/1180?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=sal%C3%A7a&sayfa=1&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=sal%C3%A7a&sayfa=2&sirala=akilli-siralama\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Salça\")\n",
    "        data=veriekle(\"Salça\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/zeytin/c/1356?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=zeytin&sayfa=1&kategori=113&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=zeytin&sayfa=2&kategori=113&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=zeytin&sayfa=3&kategori=113&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=zeytin&sayfa=4&kategori=113&sirala=akilli-siralama\"]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Zeytin\")\n",
    "        data=veriekle(\"Zeytin\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/cipsler/c/1552?q=%3AbestSeller%3Acategory%3A1552%3AinStockFlag%3Atrue&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=cips&sayfa=1&kategori=1088&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=cips&sayfa=2&kategori=1088&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=cips&sayfa=3&kategori=1088&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=cips&sayfa=4&kategori=1088&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=cips&sayfa=5&kategori=1088&sirala=akilli-siralama\",\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Cipsler\")\n",
    "        data=veriekle(\"Cipsler\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=Toz+%C5%9Eeker%3AbestSeller%3AinStockFlag%3Atrue&text=Toz+%C5%9Eeker#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=Toz%20%C5%9Eeker&sayfa=1&kategori=172\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Toz Şeker\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Toz\", regex=True)]\n",
    "            data=veriekle(\"Toz Şeker\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=k%C3%BCp+%C5%9Feker%3AbestSeller%3AinStockFlag%3Atrue&text=k%C3%BCp+%C5%9Feker#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=k%C3%BCp%20%C5%9Feker&sayfa=1&kategori=1347\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kesme Şeker\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Küp\", regex=True)]\n",
    "            data=veriekle(\"Kesme Şeker\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=re%C3%A7el%3AbestSeller%3AinStockFlag%3Atrue&text=re%C3%A7el#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=re%C3%A7el&sayfa=1&kategori=10107&sirala=akilli-siralama\",\"https://www.migros.com.tr/arama?q=re%C3%A7el&sayfa=2&kategori=10107&sirala=akilli-siralama\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Reçel\")\n",
    "        data=veriekle(\"Reçel\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/ballar/c/1362?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=bal&sayfa=1&kategori=1056&sirala=akilli-siralama\",\"https://www.migros.com.tr/arama?q=bal&sayfa=2&kategori=1056&sirala=akilli-siralama\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Bal\")\n",
    "        data=veriekle(\"Bal\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=pekmez%3AbestSeller&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=pekmez&sayfa=1&kategori=10096\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Pekmez\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Pekmez|Pekmezi\", regex=True)]\n",
    "            data=veriekle(\"Pekmez\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=tahin+helva%3AbestSeller%3AproductPrimaryCategoryCode%3A1374%3AinStockFlag%3Atrue&text=tahin+helva#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=tahin%20helvas%C4%B1&sayfa=1&kategori=10097\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Tahin Helvası\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Helvası|Helva\", regex=True)]\n",
    "            data=veriekle(\"Tahin Helvası\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=f%C4%B1nd%C4%B1k+ezmesi%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=f%C4%B1nd%C4%B1k%20ezmesi&sayfa=1&kategori=10104\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Fındık Ezmesi\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Fındık Ezmesi\", regex=True)]\n",
    "            data=veriekle(\"Fındık Ezmesi\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=tablet+%C3%A7ikolata%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=tablet%20%C3%A7ikolata\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Çikolata Tablet\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Tablet|Kare\", regex=True)]\n",
    "            data=veriekle(\"Çikolata Tablet\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=krem+%C3%A7ikolata%3AbestSeller%3AproductPrimaryCategoryCode%3A1381%3AinStockFlag%3Atrue&text=krem+%C3%A7ikolata#\"]\n",
    "        migros=[\"https://www.migros.com.tr/kakao-findik-kremalari-c-2779?sayfa=1\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Çikolata Krem\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Ezmesi\", regex=True)]\n",
    "            data=veriekle(\"Çikolata Krem\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=lokum%3AbestSeller%3AproductPrimaryCategoryCode%3A1494%3AinStockFlag%3Atrue&text=lokum#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=lokum&sayfa=1&kategori=10268&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=lokum&sayfa=2&kategori=10268&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=lokum&sayfa=3&kategori=10268&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=lokum&sayfa=4&kategori=10268&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=lokum&sayfa=5&kategori=10268&sirala=akilli-siralama\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Lokum\")\n",
    "        data=veriekle(\"Lokum\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=sak%C4%B1z%3AbestSeller%3AproductPrimaryCategoryCode%3A1501%3AinStockFlag%3Atrue&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=sak%C4%B1z&sayfa=1&kategori=1091&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=sak%C4%B1z&sayfa=2&kategori=1091&sirala=akilli-siralama\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Sakız\")\n",
    "        data=veriekle(\"Sakız\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/sekerleme/c/1494?q=%3AbestSeller%3Acategory%3ABRN-1949%3Acategory%3ABRN-2504%3Acategory%3ABRN-3185%3Acategory%3ABRN-2125%3Acategory%3ABRN-3091%3Acategory%3ABRN-2999%3AinStockFlag%3Atrue&text=#\"]\n",
    "        migros=[\"https://www.migros.com.tr/yumusak-seker-c-2818?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/yumusak-seker-c-2818?sayfa=2&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/yumusak-seker-c-2818?sayfa=3&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/draje-sekerleme-c-2816?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/draje-sekerleme-c-2816?sayfa=2&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kağıtlı Şeker\")\n",
    "        data=veriekle(\"Kağıtlı Şeker\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/kap-dondurma/c/1261?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\",\n",
    "                \"https://www.carrefoursa.com/tek-dondurma/c/1266?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/dondurma-c-41b?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/dondurma-c-41b?sayfa=2&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/dondurma-c-41b?sayfa=3&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/dondurma-c-41b?sayfa=4&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/dondurma-c-41b?sayfa=5&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/dondurma-c-41b?sayfa=6&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Dondurma\")\n",
    "        data=veriekle(\"Dondurma\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/baharat/c/1167?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=baharat&sayfa=1&kategori=10180&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=baharat&sayfa=2&kategori=10180&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=baharat&sayfa=3&kategori=10180&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=baharat&sayfa=4&kategori=10180&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=baharat&sayfa=5&kategori=10180&sirala=akilli-siralama\",\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Baharat\")\n",
    "        data=veriekle(\"Baharat\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/tuz/c/1166?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "        migros=[\"https://www.migros.com.tr/tuz-c-436?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/tuz-c-436?sayfa=2&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Tuz\")\n",
    "        data=veriekle(\"Tuz\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=kabartma%3AbestSeller%3AproductPrimaryCategoryCode%3A1282%3AinStockFlag%3Atrue&text=kabartma#\"]\n",
    "        migros=[\"https://www.migros.com.tr/kabartma-tozu-sekerli-vanilin-c-2893\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kabartma Maddeleri\")\n",
    "        data=veriekle(\"Kabartma Maddeleri\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=sirke%3AbestSeller%3AproductPrimaryCategoryCode%3A1219%3AinStockFlag%3Atrue&text=sirke#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=sirke&sayfa=1&kategori=10319&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=sirke&sayfa=2&kategori=10319&sirala=akilli-siralama\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Sirke\")\n",
    "        data=veriekle(\"Sirke\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=ket%C3%A7ap%3AbestSeller%3AinStockFlag%3Atrue&text=ket%C3%A7ap#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=ket%C3%A7ap&sayfa=1&kategori=10311\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Ketçap\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Mayonez\", regex=True)]\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Ketçap\")]\n",
    "\n",
    "            data=veriekle(\"Ketçap\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=mayonez%3AbestSeller%3AproductPrimaryCategoryCode%3A1212%3AinStockFlag%3Atrue&text=mayonez#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=mayonez&sayfa=1&kategori=10312\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Mayonez\")\n",
    "        data=veriekle(\"Mayonez\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/tahin-pekmez-helva/c/1374?q=%3AbestSeller%3Acategory%3A1310%3AinStockFlag%3Atrue&text=#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=tahin&sayfa=1&kategori=10095\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Tahin\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[urunler_df[\"Ürün\"].str.contains(\"Tahin\", regex=True)]\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Pekmezi|Helva\", regex=True)]\n",
    "            data=veriekle(\"Tahin\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/hazir-corbalar/c/1224?q=%3AbestSeller&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=haz%C4%B1r%20%C3%A7orba&sayfa=1&kategori=1103&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=haz%C4%B1r%20%C3%A7orba&sayfa=2&kategori=1103&sirala=akilli-siralama\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Hazır Çorbalar\")\n",
    "        data=veriekle(\"Hazır Çorbalar\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/hazirlanacak-tatlilar/c/1300?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/toz-tatlilar-c-287d?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/toz-tatlilar-c-287d?sayfa=2&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/toz-tatlilar-c-287d?sayfa=3&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Hazır Pakette Toz Tatlılar (Puding)\")\n",
    "        data=veriekle(\"Hazır Pakette Toz Tatlılar (Puding)\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search/?q=t%C3%BCrk+kahvesi%3AbestSeller%3AinStockFlag%3Atrue&text=t%C3%BCrk+kahvesi#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=t%C3%BCrk%20kahvesi&sayfa=1&sirala=akilli-siralama&kategori=10436\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kahve\")\n",
    "        data=veriekle(\"Kahve\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/kahve/c/1467?q=%3AbestSeller%3Acategory%3A1467%3AinStockFlag%3Atrue&text=#\"]\n",
    "        migros=[\"https://www.migros.com.tr/hazir-kahve-c-11222?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/hazir-kahve-c-11222?sayfa=2&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/hazir-kahve-c-11222?sayfa=3&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/hazir-kahve-c-11222?sayfa=4&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Hazır Kahve\")\n",
    "        if urunler_df is not None:\n",
    "            urunler_df=urunler_df[~urunler_df[\"Ürün\"].str.contains(\"Türk\", regex=True)]\n",
    "            data=veriekle(\"Hazır Kahve\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=%C3%A7ay&sayfa=1&kategori=10433&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=%C3%A7ay&sayfa=2&kategori=10433&sirala=akilli-siralama\",\n",
    "                \"https://www.migros.com.tr/arama?q=%C3%A7ay&sayfa=1&kategori=70174\",\n",
    "                \"https://www.migros.com.tr/arama?q=%C3%A7ay&sayfa=1&kategori=70175\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=\"\",migros=migros,name=\"Çay\")\n",
    "        data=veriekle(\"Çay\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        migros=[\"https://www.migros.com.tr/bitki-cayi-c-28c0?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/bitki-cayi-c-28c0?sayfa=2&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/bitki-cayi-c-28c0?sayfa=3&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=\"\",migros=migros,name=\"Bitki ve Meyve Çayı (Poşet)\")\n",
    "        data=veriekle(\"Bitki ve Meyve Çayı (Poşet)\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=kakaolu%20s%C3%BCt&sayfa=1&kategori=108\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=\"\",migros=migros,name=\"Kakaolu Toz İçecekler\")\n",
    "        data=veriekle(\"Kakaolu Toz İçecekler\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/sular/c/1411?q=%3AbestSeller%3AinStockFlag%3Atrue&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/su-c-84?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/su-c-84?sayfa=2&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Su\")\n",
    "        data=veriekle(\"Su\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/maden-sulari/c/1412?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "        migros=[\"https://www.migros.com.tr/maden-suyu-c-85?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/maden-suyu-c-85?sayfa=2&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/maden-suyu-c-85?sayfa=3&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Maden Suyu ve Sodası\")\n",
    "        data=veriekle(\"Maden Suyu ve Sodası\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=gazoz%3AbestSeller&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/gazoz-c-467?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/gazoz-c-467?sayfa=2&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/gazoz-c-467?sayfa=3&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Gazoz Meyveli\")\n",
    "        data=veriekle(\"Gazoz Meyveli\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/kola/c/1419?q=%3AbestSeller%3AinStockFlag%3Atrue&text=#\"]\n",
    "        migros=[\"https://www.migros.com.tr/kola-c-465?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/kola-c-465?sayfa=2&sirala=onerilenler\"\n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Kola\")\n",
    "        data=veriekle(\"Kola\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=so%C4%9Fuk+%C3%A7ay%3AbestSeller&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/soguk-cay-c-28be?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/soguk-cay-c-28be?sayfa=2&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/soguk-cay-c-28be?sayfa=3&sirala=onerilenler\",\n",
    "                \n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Soğuk Çay\")\n",
    "        data=veriekle(\"Soğuk Çay\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=ayran%3AbestSeller&show=All\"]\n",
    "        migros=[\"https://www.migros.com.tr/ayran-c-47a?sayfa=1\",\n",
    "                \n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Ayran\")\n",
    "        data=veriekle(\"Ayran\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=meyve+suyu%3AbestSeller%3AinStockFlag%3Atrue&text=meyve+suyu#\"]\n",
    "        migros=[\"https://www.migros.com.tr/meyve-suyu-c-46c?sayfa=1&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/meyve-suyu-c-46c?sayfa=2&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/meyve-suyu-c-46c?sayfa=3&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/meyve-suyu-c-46c?sayfa=4&sirala=onerilenler\",\n",
    "                \"https://www.migros.com.tr/meyve-suyu-c-46c?sayfa=5&sirala=onerilenler\",\n",
    "                \n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour,migros,\"Meyve Suyu\")\n",
    "        data=veriekle(\"Meyve Suyu\",data,urunler_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=tulum%20peyniri&sayfa=1&kategori=10036\",\n",
    "                \n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=\"\",migros=migros,name=\"Tulum Peyniri\")\n",
    "        data=veriekle(\"Tulum Peyniri\",data,urunler_df)\n",
    "\n",
    "\n",
    "        carrefour=[\"https://www.carrefoursa.com/search?q=kakao%3AbestSeller%3AproductPrimaryCategoryCode%3A1282%3AinStockFlag%3Atrue&text=kakao#\"]\n",
    "        migros=[\"https://www.migros.com.tr/arama?q=kakao&sayfa=1&kategori=1118\"\n",
    "                \n",
    "        ]\n",
    "        options = Options()\n",
    "        options.headless = False \n",
    "\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        urunler_df=vericek(carrefour=carrefour,migros=migros,name=\"Kakao\")\n",
    "        data=veriekle(\"Kakao\",data,urunler_df)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # Function to convert numeric columns to float and drop rows where conversion fails\n",
    "        def convert_to_float_and_drop_non_numeric(df):\n",
    "            numeric_columns = df.columns[1:]  # Exclude the 'Ürün' column\n",
    "            # Attempt to convert all numeric columns to float\n",
    "            for col in numeric_columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Drop rows where all numeric columns have NaN (i.e., non-convertible rows)\n",
    "            df_cleaned = df.dropna(subset=numeric_columns, how='all')\n",
    "            return df_cleaned\n",
    "\n",
    "        # Apply the conversion and cleaning process\n",
    "        df_cleaned = convert_to_float_and_drop_non_numeric(data.copy())\n",
    "\n",
    "        # Function to fill NaN values from both right to left and left to right\n",
    "        def fill_nan_both_directions(row):\n",
    "\n",
    "            filled_row = row[::-1].fillna(method='ffill')[::-1]\n",
    "\n",
    "            filled_row = filled_row.fillna(method='ffill')\n",
    "            return filled_row\n",
    "\n",
    "        def fill_nan_both_directions_corrected(df):\n",
    "            numeric_columns = df.columns[1:]  # Exclude the 'Ürün' column\n",
    "            df[numeric_columns] = df[numeric_columns].apply(fill_nan_both_directions, axis=1)\n",
    "            return df\n",
    "\n",
    "\n",
    "        df_filled_corrected = fill_nan_both_directions_corrected(df_cleaned)\n",
    "\n",
    "\n",
    "        ağırlıklar=pd.read_csv(\"ağırlıklar.csv\")\n",
    "        ağırlıklar=ağırlıklar.set_index(ağırlıklar[\"Ürün\"])\n",
    "        ağırlıklar=ağırlıklar.drop(\"Ürün\",axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        gfe=pd.read_csv(\"gfe.csv\")\n",
    "        gfe=gfe.set_index(pd.to_datetime(gfe[\"Tarih\"]))\n",
    "        gfe=gfe.drop(\"Tarih\",axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        data1=df_filled_corrected.copy()\n",
    "        degisim=(((data1.iloc[:,-1]/data1.iloc[:,-2])-1)*100).fillna(0).groupby(level=0).mean().sort_index()\n",
    "\n",
    "\n",
    "\n",
    "        ağırlıklar[\"Değişim\"]=degisim\n",
    "\n",
    "\n",
    "\n",
    "        ağırlıklar[f\"Endeks_{bugün}\"]=ağırlıklar[f\"Endeks_{dün}\"]*(1+(ağırlıklar[\"Değişim\"]/100))\n",
    "\n",
    "        ağırlıklar[f\"Ağırlıklı Endeks_{bugün}\"]=ağırlıklar[f\"Endeks_{bugün}\"]*ağırlıklar[\"Ağırlık\"]\n",
    "        gfe.loc[pd.to_datetime(bugün)]=ağırlıklar[f\"Ağırlıklı Endeks_{bugün}\"].sum()\n",
    "        gfe.to_csv(\"gfe.csv\",index=True)\n",
    "\n",
    "\n",
    "        endeks_sutunlari = ağırlıklar.filter(like='Endeks_')\n",
    "        endeksler = [col for col in ağırlıklar.columns if col.startswith('Endeks_')]\n",
    "        ağırlıklar[endeksler].to_csv(\"endeksler.csv\",index=True)\n",
    "\n",
    "\n",
    "        ağırlıklar.to_csv(\"ağırlıklar.csv\",index=True)\n",
    "\n",
    "\n",
    "        data1.to_csv(\"sepet.csv\")\n",
    "\n",
    "        tarih=datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "        tarih=pd.DataFrame({\"Current DateTime\": [tarih]})\n",
    "        tarih.to_csv(\"tarih.csv\")\n",
    "\n",
    "        import os\n",
    "        import subprocess\n",
    "        from datetime import datetime\n",
    "        import time\n",
    "        import git\n",
    "        from git import Repo\n",
    "        import os\n",
    "        repo_dir = \".git\"  # Buraya Git deposunun yolunu girin\n",
    "\n",
    "        def git_add_commit_push():\n",
    "            try:\n",
    "                # Repo nesnesini oluştur\n",
    "                repo = Repo(repo_dir)\n",
    "                assert not repo.bare\n",
    "\n",
    "                # Git add: tüm değişiklikleri ekliyoruz\n",
    "                repo.git.add(A=True)  # A=True ile tüm dosyalar eklenir\n",
    "\n",
    "                # Commit işlemi\n",
    "                commit_message = \"update\"\n",
    "                repo.index.commit(commit_message)\n",
    "                print(f\"Commit işlemi başarılı: {commit_message}\")\n",
    "\n",
    "                # Push işlemi\n",
    "                origin = repo.remote(name='origin')\n",
    "                origin.push()\n",
    "                print(\"Push işlemi başarılı.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Git işlemi sırasında hata oluştu: {e}\")\n",
    "\n",
    "            # Ana fonksiyonu çağırma\n",
    "        git_add_commit_push()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred:\")\n",
    "        time.sleep(5)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ürün</th>\n",
       "      <th>2024-10-11</th>\n",
       "      <th>2024-10-12</th>\n",
       "      <th>2024-10-13</th>\n",
       "      <th>2024-10-14</th>\n",
       "      <th>2024-10-15</th>\n",
       "      <th>2024-10-16</th>\n",
       "      <th>2024-10-17</th>\n",
       "      <th>2024-10-18</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Antep Fıstığı</th>\n",
       "      <td>Tadım Antep Fıstığı 180 G</td>\n",
       "      <td>187.95</td>\n",
       "      <td>187.95</td>\n",
       "      <td>187.95</td>\n",
       "      <td>187.95</td>\n",
       "      <td>187.95</td>\n",
       "      <td>187.95</td>\n",
       "      <td>187.95</td>\n",
       "      <td>187.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Antep Fıstığı</th>\n",
       "      <td>Siirt Fıstığı Tuzlu Kavrulmuş Kg</td>\n",
       "      <td>899.95</td>\n",
       "      <td>899.95</td>\n",
       "      <td>899.95</td>\n",
       "      <td>899.95</td>\n",
       "      <td>899.95</td>\n",
       "      <td>899.95</td>\n",
       "      <td>899.95</td>\n",
       "      <td>899.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Antep Fıstığı</th>\n",
       "      <td>Migros Kavrulmuş Antep Fıstığı 150 G</td>\n",
       "      <td>89.50</td>\n",
       "      <td>89.50</td>\n",
       "      <td>89.50</td>\n",
       "      <td>89.50</td>\n",
       "      <td>89.50</td>\n",
       "      <td>89.50</td>\n",
       "      <td>89.50</td>\n",
       "      <td>89.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Antep Fıstığı</th>\n",
       "      <td>Migros Antep Fıstığı Jumbo 200 G</td>\n",
       "      <td>123.00</td>\n",
       "      <td>123.00</td>\n",
       "      <td>123.00</td>\n",
       "      <td>123.00</td>\n",
       "      <td>123.00</td>\n",
       "      <td>123.00</td>\n",
       "      <td>123.00</td>\n",
       "      <td>123.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Antep Fıstığı</th>\n",
       "      <td>Master Nut Kabuklu Antep Fıstığı 140 G</td>\n",
       "      <td>132.95</td>\n",
       "      <td>132.95</td>\n",
       "      <td>132.95</td>\n",
       "      <td>132.95</td>\n",
       "      <td>132.95</td>\n",
       "      <td>132.95</td>\n",
       "      <td>132.95</td>\n",
       "      <td>132.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Şehriye</th>\n",
       "      <td>Barilla Arpa Şehriye Risoni 500 g</td>\n",
       "      <td>27.71</td>\n",
       "      <td>27.71</td>\n",
       "      <td>27.71</td>\n",
       "      <td>27.71</td>\n",
       "      <td>27.71</td>\n",
       "      <td>27.71</td>\n",
       "      <td>27.71</td>\n",
       "      <td>27.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Şehriye</th>\n",
       "      <td>Filiz Tel Şehriye 500 G</td>\n",
       "      <td>22.75</td>\n",
       "      <td>22.75</td>\n",
       "      <td>22.75</td>\n",
       "      <td>22.75</td>\n",
       "      <td>22.75</td>\n",
       "      <td>22.75</td>\n",
       "      <td>22.75</td>\n",
       "      <td>22.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Şehriye</th>\n",
       "      <td>Pastavilla İri Arpa Şehriye 500 Gr</td>\n",
       "      <td>27.90</td>\n",
       "      <td>27.90</td>\n",
       "      <td>27.90</td>\n",
       "      <td>27.90</td>\n",
       "      <td>27.90</td>\n",
       "      <td>27.90</td>\n",
       "      <td>27.90</td>\n",
       "      <td>27.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Şehriye</th>\n",
       "      <td>Pastavilla Tel Şehriye 500 G</td>\n",
       "      <td>27.95</td>\n",
       "      <td>27.95</td>\n",
       "      <td>27.95</td>\n",
       "      <td>27.95</td>\n",
       "      <td>27.95</td>\n",
       "      <td>27.95</td>\n",
       "      <td>27.95</td>\n",
       "      <td>27.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Şehriye</th>\n",
       "      <td>Migros Tel Şehriye 500 G</td>\n",
       "      <td>10.75</td>\n",
       "      <td>10.75</td>\n",
       "      <td>10.75</td>\n",
       "      <td>10.75</td>\n",
       "      <td>10.75</td>\n",
       "      <td>10.75</td>\n",
       "      <td>10.75</td>\n",
       "      <td>11.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7658 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Ürün  2024-10-11  2024-10-12  \\\n",
       "                                                                                \n",
       "Antep Fıstığı               Tadım Antep Fıstığı 180 G      187.95      187.95   \n",
       "Antep Fıstığı        Siirt Fıstığı Tuzlu Kavrulmuş Kg      899.95      899.95   \n",
       "Antep Fıstığı    Migros Kavrulmuş Antep Fıstığı 150 G       89.50       89.50   \n",
       "Antep Fıstığı        Migros Antep Fıstığı Jumbo 200 G      123.00      123.00   \n",
       "Antep Fıstığı  Master Nut Kabuklu Antep Fıstığı 140 G      132.95      132.95   \n",
       "...                                               ...         ...         ...   \n",
       "Şehriye             Barilla Arpa Şehriye Risoni 500 g       27.71       27.71   \n",
       "Şehriye                       Filiz Tel Şehriye 500 G       22.75       22.75   \n",
       "Şehriye            Pastavilla İri Arpa Şehriye 500 Gr       27.90       27.90   \n",
       "Şehriye                  Pastavilla Tel Şehriye 500 G       27.95       27.95   \n",
       "Şehriye                      Migros Tel Şehriye 500 G       10.75       10.75   \n",
       "\n",
       "               2024-10-13  2024-10-14  2024-10-15  2024-10-16  2024-10-17  \\\n",
       "                                                                            \n",
       "Antep Fıstığı      187.95      187.95      187.95      187.95      187.95   \n",
       "Antep Fıstığı      899.95      899.95      899.95      899.95      899.95   \n",
       "Antep Fıstığı       89.50       89.50       89.50       89.50       89.50   \n",
       "Antep Fıstığı      123.00      123.00      123.00      123.00      123.00   \n",
       "Antep Fıstığı      132.95      132.95      132.95      132.95      132.95   \n",
       "...                   ...         ...         ...         ...         ...   \n",
       "Şehriye             27.71       27.71       27.71       27.71       27.71   \n",
       "Şehriye             22.75       22.75       22.75       22.75       22.75   \n",
       "Şehriye             27.90       27.90       27.90       27.90       27.90   \n",
       "Şehriye             27.95       27.95       27.95       27.95       27.95   \n",
       "Şehriye             10.75       10.75       10.75       10.75       10.75   \n",
       "\n",
       "               2024-10-18  \n",
       "                           \n",
       "Antep Fıstığı      187.95  \n",
       "Antep Fıstığı      899.95  \n",
       "Antep Fıstığı       89.50  \n",
       "Antep Fıstığı      123.00  \n",
       "Antep Fıstığı      132.95  \n",
       "...                   ...  \n",
       "Şehriye             27.71  \n",
       "Şehriye             22.75  \n",
       "Şehriye             27.90  \n",
       "Şehriye             27.95  \n",
       "Şehriye             11.75  \n",
       "\n",
       "[7658 rows x 9 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv(\"sepet.csv\")\n",
    "data=data.set_index(data[\"Unnamed: 0\"]).drop(\"Unnamed: 0\",axis=1)\n",
    "data.index.name=\"\"\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GFE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tarih</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-10-11</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-12</th>\n",
       "      <td>100.274305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-13</th>\n",
       "      <td>100.167222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-14</th>\n",
       "      <td>100.737845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-15</th>\n",
       "      <td>100.930904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-16</th>\n",
       "      <td>101.225035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-17</th>\n",
       "      <td>101.368670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-18</th>\n",
       "      <td>101.174089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   GFE\n",
       "Tarih                 \n",
       "2024-10-11  100.000000\n",
       "2024-10-12  100.274305\n",
       "2024-10-13  100.167222\n",
       "2024-10-14  100.737845\n",
       "2024-10-15  100.930904\n",
       "2024-10-16  101.225035\n",
       "2024-10-17  101.368670\n",
       "2024-10-18  101.174089"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gfe=pd.read_csv(\"gfe.csv\")\n",
    "gfe=gfe.set_index(pd.to_datetime(gfe[\"Tarih\"]))\n",
    "gfe=gfe.drop(\"Tarih\",axis=1)\n",
    "gfe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ağırlıklar=pd.read_csv(\"ağırlıklar.csv\")\n",
    "ağırlıklar=ağırlıklar.set_index(ağırlıklar[\"Ürün\"])\n",
    "ağırlıklar=ağırlıklar.drop(\"Ürün\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Antep Fıstığı      0.000000\n",
       "Armut              0.000000\n",
       "Ay Çekirdeği       0.000000\n",
       "Ayran              0.000000\n",
       "Ayva               0.000000\n",
       "                     ...   \n",
       "Çikolata Tablet    0.000000\n",
       "Çilek              0.000000\n",
       "Üzüm               0.000000\n",
       "Şeftali            0.000000\n",
       "Şehriye            1.283079\n",
       "Length: 128, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degisim=(((data.iloc[:,8]/data.iloc[:,7])-1)*100).fillna(0).groupby(level=0).mean().sort_index()\n",
    "degisim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ağırlıklar[\"Değişim\"]=degisim\n",
    "\n",
    "\n",
    "\n",
    "ağırlıklar[\"Endeks_2024-10-18\"]=ağırlıklar[\"Endeks_2024-10-17\"]*(1+(ağırlıklar[\"Değişim\"]/100))\n",
    "\n",
    "ağırlıklar[\"Ağırlıklı Endeks_2024-10-18\"]=ağırlıklar[\"Endeks_2024-10-18\"]*ağırlıklar[\"Ağırlık\"]\n",
    "\n",
    "\n",
    "gfe.loc[pd.to_datetime(\"2024-10-18\")]=ağırlıklar[\"Ağırlıklı Endeks_2024-10-18\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GFE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tarih</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-10-11</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-12</th>\n",
       "      <td>100.274305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-13</th>\n",
       "      <td>100.167060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-14</th>\n",
       "      <td>100.735877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-15</th>\n",
       "      <td>100.929103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-16</th>\n",
       "      <td>101.222220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-17</th>\n",
       "      <td>101.365852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-18</th>\n",
       "      <td>101.170860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   GFE\n",
       "Tarih                 \n",
       "2024-10-11  100.000000\n",
       "2024-10-12  100.274305\n",
       "2024-10-13  100.167060\n",
       "2024-10-14  100.735877\n",
       "2024-10-15  100.929103\n",
       "2024-10-16  101.222220\n",
       "2024-10-17  101.365852\n",
       "2024-10-18  101.170860"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfe.to_csv(\"gfe.csv\",index=True)\n",
    "\n",
    "\n",
    "endeks_sutunlari = ağırlıklar.filter(like='Endeks_')\n",
    "endeksler = [col for col in ağırlıklar.columns if col.startswith('Endeks_')]\n",
    "ağırlıklar[endeksler].to_csv(\"endeksler.csv\",index=True)\n",
    "\n",
    "\n",
    "ağırlıklar.to_csv(\"ağırlıklar.csv\",index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"sepet.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
